{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H67AFi16O6Xs"
   },
   "source": [
    "# environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l3dV1zolwNgc",
    "outputId": "5193e29d-45cd-417a-b487-379bcf1bd7c9"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch, sys\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Torch:\", torch.__version__ if hasattr(torch, \"__version__\") else \"Not installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDRJ5Ao_wUqM",
    "outputId": "a80fb990-eb13-496b-fece-998e50475085"
   },
   "outputs": [],
   "source": [
    "# Fresh start\n",
    "%pip -q install --upgrade pip\n",
    "\n",
    "# Install PyTorch CUDA 12.4 wheels\n",
    "%pip -q install --index-url https://download.pytorch.org/whl/cu124 torch torchvision torchaudio\n",
    "import torch, platform\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available(), \"| Device cap:\", torch.cuda.get_device_capability() if torch.cuda.is_available() else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pG59nNI7wYUC"
   },
   "outputs": [],
   "source": [
    "%pip -q install unsloth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWVv8_s_wvaY",
    "outputId": "8863ef16-54ce-4257-8f95-6de158d905c3"
   },
   "outputs": [],
   "source": [
    "# For Torch 2.5 + CUDA 12.4 wheels installed above:\n",
    "%pip -q install \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z77qc_PDw2fW",
    "outputId": "22bd4a2c-0e92-4489-a322-95d1515f0f96"
   },
   "outputs": [],
   "source": [
    "!wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A8Se2wFrw9Vk",
    "outputId": "5174dffc-bcd5-4c8f-c6f0-eba1bb50846d"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "%pip install --upgrade --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth-zoo.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "id": "U8l7A0PKxGVT",
    "outputId": "8ec75747-0ee6-4adc-b00b-6bc19ac522f7"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, FastModel\n",
    "import torch\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "# Tiny public jsonl for a quick smoke test\n",
    "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": url}, split=\"train\")\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b\",   # pick any supported model\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,                  # QLoRA 4-bit\n",
    "    load_in_8bit = False,\n",
    "    load_in_16bit = False,\n",
    "    full_finetuning = False,\n",
    "    # token=\"hf_...\"                       # if using gated models\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    max_seq_length = max_seq_length,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    args = SFTConfig(\n",
    "        max_seq_length = max_seq_length,\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10,\n",
    "        max_steps = 60,           # use num_train_epochs=1..3 for a real run\n",
    "        logging_steps = 1,\n",
    "        output_dir = \"outputs\",\n",
    "        optim = \"adamw_8bit\",\n",
    "        seed = 3407,\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25KydEjaxqK5",
    "outputId": "2fe757f8-e538-4b6d-859f-3eeb55a3c0e0"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import sys; print(\"Python:\", sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lmmzJfvxt3l",
    "outputId": "81198dbd-c1fd-4b2f-f88a-61fe3e0e7a34"
   },
   "outputs": [],
   "source": [
    "%pip -q install --upgrade pip\n",
    "%pip -q install --index-url https://download.pytorch.org/whl/cu124 torch torchvision torchaudio\n",
    "import torch; print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda, \"| GPU available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TH5_WYy5yNCy",
    "outputId": "a66cd1da-08af-4509-860d-10989952c7da"
   },
   "outputs": [],
   "source": [
    "# 0) Info (optional)\n",
    "!nvidia-smi\n",
    "import sys; print(\"Python:\", sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5evB8RMyQe-",
    "outputId": "3779f983-7eea-4d8b-aa18-5d3561bffc82"
   },
   "outputs": [],
   "source": [
    "# 1) Torch (CUDA 12.4 wheels are stable on Colab)\n",
    "%pip -q install --upgrade pip\n",
    "%pip -q install --index-url https://download.pytorch.org/whl/cu124 torch torchvision torchaudio\n",
    "import torch; print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda, \"| GPU:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-K0LXx200PtP",
    "outputId": "e9d6df16-d314-464c-be15-90cc26aabc34"
   },
   "outputs": [],
   "source": [
    "# Create a clean virtual environment\n",
    "!python -m venv /content/unsloth-venv\n",
    "# Upgrade pip inside the venv\n",
    "!/content/unsloth-venv/bin/python -m pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naobOosz0a1_"
   },
   "outputs": [],
   "source": [
    "!rm -rf /content/unsloth-venv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mcF8R3k0e-2"
   },
   "outputs": [],
   "source": [
    "!python -m venv /content/unsloth-venv --without-pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mwKGLaq00i6Y",
    "outputId": "805f6284-e2a0-42f0-a925-1449b144aa20"
   },
   "outputs": [],
   "source": [
    "!curl -sS https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
    "!/content/unsloth-venv/bin/python get-pip.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TIrqyQYN01vT",
    "outputId": "45267444-d1c8-4384-f270-b481dae288cd"
   },
   "outputs": [],
   "source": [
    "# Torch (CUDA 12.4)\n",
    "!/content/unsloth-venv/bin/python -m pip install --index-url https://download.pytorch.org/whl/cu124 torch==2.5.1+cu124\n",
    "\n",
    "# Unsloth + dependencies\n",
    "!/content/unsloth-venv/bin/python -m pip install \\\n",
    "  \"trl==0.23.0\" \"transformers==4.56.2\" \\\n",
    "  \"accelerate>=0.34.2\" \"peft>=0.13.2\" \"bitsandbytes>=0.45.0\" \"datasets>=2.20.0\" \\\n",
    "  unsloth unsloth_zoo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8KoF-uuK2zBL",
    "outputId": "2d621129-6153-44e0-f8fd-9e5f224499c5"
   },
   "outputs": [],
   "source": [
    "import os, sys, glob, site\n",
    "\n",
    "# Path to your venv\n",
    "venv_path = \"/content/unsloth-venv\"\n",
    "\n",
    "# Compute the venv's site-packages folder\n",
    "pyver = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "site_dir = f\"{venv_path}/lib/python{pyver}/site-packages\"\n",
    "\n",
    "# If folder name differs (e.g., python3.10 vs 3.12), auto-detect it\n",
    "cands = glob.glob(f\"{venv_path}/lib/python*/site-packages\")\n",
    "if not os.path.isdir(site_dir) and cands:\n",
    "    site_dir = cands[0]\n",
    "\n",
    "# Add it so imports come from the venv\n",
    "site.addsitedir(site_dir)\n",
    "print(\"✅ Added venv to sys.path:\", site_dir)\n",
    "\n",
    "# ---- Sanity check ----\n",
    "import torch, transformers, trl, unsloth\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"TRL:\", trl.__version__)\n",
    "print(\"Unsloth imported OK from:\", unsloth.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NNtw_xzf3TB6",
    "outputId": "80b3d495-4ff3-4574-e2d5-9c210f253ef2"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "de2e09a95e85431ba37b6bc21eecb79e",
      "0b67fab723394fd0a9a31ad44f5b3c94",
      "49093e1e518c46658bbb185682b79343",
      "86c43d7daff140488069d12276693a3d",
      "39f5135340ce4feaaf6d9ada7e79c444",
      "1afe9dae6f694328864091c19ccd3fe0",
      "e5702a8430964ffa8b2926437863a3c7",
      "1e7c0fd99b3b44148732b7867ada10f6",
      "18ab4abf681d4833b2cf03671e1e0694",
      "aa7f4cd8aab54786b1aaf261dd2a2ea7",
      "9b400e46be9142bbb60a59cfd561b820",
      "12d5ec8fa20f40a5864f3107ea29b6cf",
      "f6021be024ed48a5b31e59f0b043354a",
      "117a8220786b4cf98993fbc71b38a56d",
      "ca1668e4eb774208b08b61dc47155da1",
      "8b3a2a6ede97490cabfeb2418671ecc1",
      "8ea4dadd7aa24b7e914f1d75df456b92",
      "c3e9f475e24148dfbe1700b6f96f6e3c",
      "4ccbe2cc5ade4daf9189c211bc27b040",
      "981f2778751046d0899d8a5975aa33a8",
      "9768ee3b2458478f86bd87e087f2c41d",
      "012f35f0f1614b9ea80a05586c76b741",
      "edd25a026422433fa68f43bdc660bf2e",
      "004d6b31336749d9b898fa2303a47b48",
      "1cf5452759764b9d88dedf60fc2a8b85",
      "91c02088afe047009471cb6be94b9d87",
      "37bed567aeed49619fcfa0fe44ba1eec",
      "4f820fb488c34612a219f4acfe1f7bdb",
      "3f05c482366e4dec8ca479ee0707098f",
      "e2ca6cbf04a14a48a02579bf1dc33a68",
      "8ee4f5b1c19245aebbbe413c60c6e2a6",
      "9c0166654bf040a09824ae244eb540fb",
      "9b423eb2c4eb45fd8da6991b21a198b1",
      "4ae7111a1f5840c780146799146e68cc",
      "55c3e94fd2b14e0ab655249203f2cf1d",
      "62e65e291b5142a9b3c8e5505cbb9a0a",
      "9eaf2355e2a04504a7222c0abe48a335",
      "f7a3cd10b39346f8af069e2da7e3c413",
      "371fc830de4746b2905f5ba9fb1f4377",
      "2ab3cd75febe45ecaa64048952789d91",
      "d7a898df9ca1445b8ed068eec58935cc",
      "f9e1243931c8469b95c110a85add54e2",
      "4e3a8032728f4f3c84f164863e4473bb",
      "f98eb6e71e4b4729be6ca4b435ef9d73",
      "48b4c03281d949cb9012244ea8631067",
      "f9b472e3fc1c4e88b567ef36f4a11740",
      "4b4eae0999604d59bac9f58d9bf1057f",
      "24580d91641b491bae48865ce2da6b71",
      "01dd3d9ee9da4384893a322fc0efd32b",
      "63a51807ce1944cdb58d13e24dfb9f7b",
      "ade03e6f5bbb498caf90a6959b492805",
      "cf15fbb176424006a2e168a5741b8de3",
      "82110552e31d4be7938325431747bbfe",
      "a836f363b7df4e899d80f1e73ea3c846",
      "e9e667f0b1734aeb8f830ac63fd34fff",
      "473e986ca8f04b3ca2028ca25e553a56",
      "949fa09f0b7547a89df8997e25355d0a",
      "60ef53c91e99475fbd7ca4209c9b6620",
      "62cad3cc6e344187a1bb1ce6c5b88243",
      "371c85fdfff54368a60bcf982ca1594f",
      "77a3fff0354b47e6bbe6d1c9343b5e0a",
      "5eb993c5b9804edf9fa5f7c85d470e0d",
      "a2b9ed9cca324ba4a21a3c814312dbf8",
      "c48c71ef950645199b68149850745d68",
      "b79930ab442a4e5aaf1a6a43427a1586",
      "d00c3c3837d542d691ec892c94e0a6e9",
      "4f0bc2872c6e4379b6c4f8acbd46de7b",
      "2b86f7eff6fc4e7daa2378057d3b1425",
      "5253c72e024843b19cc95045fc765484",
      "6616527a98eb4951af02a89fa5cf39ce",
      "1f31a0ddb6934e30bb0c7ab27f3e8f3f",
      "f831bbe9fa0a449c8c9bc88e93ea4b53",
      "3875e7df2595445c942e5f7fbf138700",
      "60f30dc4ea44489cb67a8743cab89225",
      "c29b80eb235a414782428cde3bac6b7e",
      "a710aab8eb254cbaa1a0472b7a7e7fc5",
      "f794b046b0614d4795e37003fa538344",
      "c9f6ec5d2e3f40a0ae6b2f3b4feed4eb",
      "7c1d93b6df694dc1ae3190714e7677d0",
      "9f98811bbb7f42459a98ea92e0ce4680",
      "57acbb3baf22424d800bd7e3888d1f68",
      "9da67dc94c3a4e75ac90ad61bca693b2",
      "b8ae3db6104b4827925e795a18d4a29b",
      "49b0213db52b4e17bd3e95b67124d9a8",
      "aebb910f432847cca9618bb819006c72",
      "c6b0f4b5df4c4eb0a2a58448de1c19e1",
      "e8eb2ba8a56c4806ac33264b9c4eb7d5",
      "4c5b58bc49054cc89296e0f6860e4cef",
      "599d2dfe45a845a6834a18ef7bd91c90",
      "a473014ff65b4e448c65da8c6568feec",
      "fbca77fe7353453f92ff18993f7d617d",
      "733e12d8b9bf40448b2db1e5a8e612f0",
      "d4d61d92bf5440ae996d449ae4793a92",
      "166dbc8901ad486fa4c6a61d1e93461b",
      "81ef15c658dc44fbb25725c2c5d8db9a",
      "d9d80a9b6abf458b824702de219a0f8c",
      "331d244437ee4d13ada5480fefa7489a",
      "42ceb9316a40475988b6e3d8bacc6477",
      "1bb60b63080e44e888ea7ceb7e59c77c",
      "1efc69d7422b434cab665a9f79b248a7",
      "f975306dfb9a4fe5abc8577e85153aed",
      "fc51af19a68446479e5b20d1c5a1020c",
      "e4ebe9a6573046b98ad7f6ad75310693",
      "421d48103ddb4947ae0768c02d2b151d",
      "8f0a5b1337ea42d99aba055dbc0c4621",
      "8c10bd6805dd44818fe372516bba0d2d",
      "83869d8b787b490c9c2cb4c94805c492",
      "1839ff9519ef452092b03b9a12111f80",
      "4916669c81b84a64937c4efdfb768545",
      "88347d0b5e1f44409fb64030cecba68f",
      "c76267f965ff44f0b39740c93823d870",
      "ec2d72c5d1814b2eb6118b75d18346c1",
      "6ed7e9d180e54454bdaaa60e393dd073",
      "6f1cb53573bc48efa8273431db687c70",
      "18ec5f3e7eb94dd48de28ddcff576fde",
      "b2e4a759d0a74b4db47a6f91ef8c0087",
      "5ad31756a4a34bda8e24772616c4194d",
      "7ed66be82ce047cdbaf641259f9fc85a",
      "510b5d139a4d4df49443ed4c877ee3cb",
      "3a97b7f65e5248f1ab9221309defbe96",
      "3862a99b114948138b6016c276b59342",
      "e3597b4cf55f4a9f9e4f787f5cefbdc0",
      "9c382c156a0b4745a87306b56afab2a3",
      "a74dd6b00d344bae80f4950335e23ce6",
      "0a4e842dbb144ae1a421f10d6de73a23",
      "91ce79c6f774480cae51389190312c1a",
      "794b4d3a6a2a4739a8bc13d9900373de",
      "e56ad885e406476588d6d0a04c86a9c4",
      "0d701f75ba5d45fca2b536facd222832",
      "0f7b1729b01644fe818cd6d7a12c0487",
      "a58b344fb4e641adb9a67947515d0723",
      "67a27993808141d49c05ef70c0281681",
      "b680576f5a6e466d98b16822456ae65e",
      "628eb1c139f943d4b6edc5d64dc31b3f",
      "d5652761c4c94edabf8df06a415554c7",
      "8279c58f82e84a6f83d7a7e0a29ad4e3",
      "61fd3aee68954f969abcc54d14bc86d4",
      "614f9a7692394f9aa4e0c13e10344c51",
      "dd47f6302ce54adc9fe2ce68f0054be5",
      "f4ad525c83e24eae9bd9836d73a88b28",
      "8a5ec9614180482e82299e341adbee88",
      "d10c859ce88846879f5bee3d52de0a0b",
      "787ed50f06814fefbf4b821462d2d849",
      "5600f42bebb74c58b183e0e6896ed404",
      "b674faab99444c75bbab9fea3d692199",
      "8ece7a7602ce48b7be3096a62a54d243",
      "7dbe700b68db48248851288e6189cd07",
      "76044ce182f14e6ca71be9af22736372",
      "e992df2eb4fe4b46b97d7274fec4960e",
      "a2d187760bfa49df936654b527b7e6e8",
      "427cea365f78416f831fad8bdde7837e",
      "8e84f84106704553972c0381f289c756",
      "705bef4e506240a181277a82b5ec7b95",
      "a910885c103f48d6911549c3ecb4524e",
      "a5ec9799d93d48a29568e79dec81d704",
      "a4b7b74364114b4980ea21619ace148e",
      "2921633f566d41ef9986a143ae923828",
      "708d9fe1a0bf442dbfa3e213005bbf37",
      "4a0214bb8c264f07a9eb9fb851d2724d",
      "5e7ecc1c19084bbc95b25c3bb39c8f15",
      "7a21d246c5874e229cb53f40a8530424",
      "f6de0f623526469eb34cad84c79f5e34",
      "8dd8a0d0e0194b17b0998121e27ed806",
      "c5128469b68c40aa974867e6559282eb",
      "35a959a8593e4c1bbbacbf48c09073e5"
     ]
    },
    "id": "dPVpXpq73qnf",
    "outputId": "c62caf18-f6d2-43ce-b6ef-334cb39abbf1"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, FastModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "max_seq_length = 1024\n",
    "\n",
    "# Small public dataset for a quick test\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"},\n",
    "    split=\"train[:200]\"\n",
    ")\n",
    "\n",
    "# Load base model (you can later change to Llama, Gemma, etc.)\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name      = \"unsloth/gpt-oss-20b\",\n",
    "    max_seq_length  = max_seq_length,\n",
    "    load_in_4bit    = True,      # fits on T4\n",
    "    full_finetuning = False,\n",
    ")\n",
    "\n",
    "# Apply LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "\n",
    "# Configure trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    args=SFTConfig(\n",
    "        output_dir=\"outputs\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=40,      # quick run\n",
    "        logging_steps=2,\n",
    "        optim=\"adamw_8bit\",\n",
    "        seed=3407,\n",
    "        max_seq_length=max_seq_length,\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304,
     "referenced_widgets": [
      "0fb13c1419b846e5b6aa5a8a5563af70",
      "826168ae1d7c4a41a877070dab8fb466",
      "7a3244bf06c94327bdabad32156625e3",
      "165379010fec4d83880f82b13240f1fe",
      "f10cc7375e0d4eeda81360a8e03d28d2",
      "1e6ed52a5e384f8bb378a28a45b9a924",
      "454e7e14eae74a6a882adcfda32bc648",
      "c9c0f4bd75df46bfae762e68693dc96d",
      "3364feffb888426282380b5516d33c23",
      "50d63da36fa745828c851be2c5e50ed7",
      "fb55039ee2ab4979a9888d806099725e",
      "bf2fa0bb3c43479f83c0cbd2e7655890",
      "cd65b300aa6644d5b573a6e8936fa9dc",
      "fa394b4a03934258ac5804500b8a5e35",
      "5e75763a66a144b8ada3d50dc0832475",
      "952ae3647439437098c03d1d8d4351aa",
      "8eb0929259d84402827c1826df1bbee0",
      "0a313c4b41aa4adcb59b3710311e99b6",
      "9560169c073d4f309af30c4d9337f38c",
      "7322d4b790f34f26b518ccae1046da94",
      "574c16046b9d4d01a69d9471c2b5b979",
      "53d8c9048a8b49e2826f93be423c848e",
      "d1c9833139fa4e97b69b8c2a15367182",
      "4b536c88059f48c3b1406aaa46b044dd",
      "1f08a42fceca4db99c059b07d0dd9f7a",
      "9757ddc326034d1e89f0761b093d085a",
      "ef6e6a5e5c7a43758518de4ec7a685f3",
      "83259a68d15d43c0a5b809499478d818",
      "c51f05f0115447c9907f75e08d167abf",
      "43c52fd15397494f947b25eca9b1febf",
      "f2e86606f1754da7b3b9d0e346cfceec",
      "8bbad7d595b040768b42e374901a420d",
      "580e82578cb544adb7ce7204c43dc64a",
      "d854ff02160e4f05bf42a5c8fbd557cc",
      "6b89d60dc4544ea0af36a1e894c82071",
      "ff3f57aebac3419191a13fb01dd2a959",
      "da09536929fe44338b5be0bcb39e0ef2",
      "fc48b51f0f9c445da0230aa779496c92",
      "9720bb2dbc1a433ea5897efd2452c846",
      "defa7baaa4b14bc0b41f2777c38b9f15",
      "82183f66c9034fd4979273e14f45519c",
      "02eb63d09a7749beaecc7c7283cfe1d3",
      "8e7d62e9331f4a8bbbabf03dcd8d56b3",
      "8261291fa125485e94bcdf3f8f71bdab",
      "adff13eced264d29bed2e9ae435b2572",
      "0be2add427424492b020c6b2e21e3d4d",
      "79d5892609db4d64b8bf31171f6d08b4",
      "71527b7c8c9b4fa585f56fd9b783e313",
      "e5de47a060a34194a0da7bbbf94adb70",
      "3aa90977991a44c2b9ad9e313ca834ff",
      "c9e647e0eb47424a852f5045b47e8d2c",
      "39a599e3538d4c33b4afd86485317fc7",
      "c130eac8cc084903aea112474e40e2a6",
      "2d3d5c7063774196a8c9e580655261ca",
      "29cdeec270d841608be1960c2d3a95d6"
     ]
    },
    "id": "r-5ffyX88RYE",
    "outputId": "2f129145-67b2-4258-823d-94bafbc10bf1"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:5000]\")  # take small slice for speed\n",
    "ds = ds.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds, eval_ds = ds[\"train\"], ds[\"test\"]\n",
    "\n",
    "def format_alpaca(example):\n",
    "    instruction = example[\"instruction\"].strip()\n",
    "    input_text = example[\"input\"].strip()\n",
    "    output = example[\"output\"].strip()\n",
    "    if input_text:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output}\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "train_data = train_ds.map(format_alpaca, remove_columns=train_ds.column_names)\n",
    "eval_data  = eval_ds.map(format_alpaca,  remove_columns=eval_ds.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H48QdH3r_E9k",
    "outputId": "645a8597-18c1-41ed-cb85-231029c2521e"
   },
   "outputs": [],
   "source": [
    "!/content/unsloth-venv/bin/python -m pip install unsloth unsloth_zoo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hf1brjLZ_XY9",
    "outputId": "5657296f-ac96-456a-f845-901c7e43b269"
   },
   "outputs": [],
   "source": [
    "# === Recreate Unsloth venv cleanly ===\n",
    "!python -m venv /content/unsloth-venv --without-pip\n",
    "!curl -sS https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
    "!/content/unsloth-venv/bin/python get-pip.py\n",
    "!/content/unsloth-venv/bin/python -m pip install --upgrade pip setuptools wheel\n",
    "\n",
    "# Install PyTorch (CUDA 12.4 wheel)\n",
    "!/content/unsloth-venv/bin/python -m pip install --index-url https://download.pytorch.org/whl/cu124 torch==2.5.1+cu124\n",
    "\n",
    "# Install Unsloth + compatible stack\n",
    "!/content/unsloth-venv/bin/python -m pip install \\\n",
    "  \"trl==0.23.0\" \"transformers==4.56.2\" \\\n",
    "  \"accelerate>=0.34.2\" \"peft>=0.13.2\" \"bitsandbytes>=0.45.0\" \"datasets>=2.20.0\" \\\n",
    "  unsloth unsloth_zoo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fOyu9jlg_tC-",
    "outputId": "c1d0c850-e088-4556-9aa1-4c9a33e9b682"
   },
   "outputs": [],
   "source": [
    "import os, sys, glob, site\n",
    "\n",
    "venv_path = \"/content/unsloth-venv\"\n",
    "pyver = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "site_dir = f\"{venv_path}/lib/python{pyver}/site-packages\"\n",
    "cands = glob.glob(f\"{venv_path}/lib/python*/site-packages\")\n",
    "if not os.path.isdir(site_dir) and cands:\n",
    "    site_dir = cands[0]\n",
    "site.addsitedir(site_dir)\n",
    "print(\"✅ Added venv to sys.path:\", site_dir)\n",
    "\n",
    "# Sanity check\n",
    "import torch, transformers, trl, unsloth\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"TRL:\", trl.__version__)\n",
    "print(\"Unsloth imported OK from:\", unsloth.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYeU6R1GB2X_"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, FastModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "# ... rest of the fine-tuning code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-M-SOASOq8A"
   },
   "source": [
    "# Full Finetuning (smollm2-135m) on Alpaca subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jaolz9xSCOfw",
    "outputId": "f8f8cff7-3e87-4cd7-ca8c-ee0e0bfa6a57"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch; print(\"CUDA:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338,
     "referenced_widgets": [
      "0a1454e50d9a49759f319e2f97c31880",
      "a77c1c55baea44c6b2415b53b04019df",
      "a290927327ce4ad7b2775d8e044fbed2",
      "6fa773e7214945a589a43bab901025f2",
      "c049ca5c10e54507b8c9a4a236d2cd78",
      "bfaf034f7d3f4fcaaf6e2642de034573",
      "f645e7772bb1465a95fce6b86d6595e9",
      "592f7ffb11e042d7b1ac5f3203711933",
      "1d0ea335685b4e2eb7d9b30936d4e06e",
      "7a3a38b75fd54eceb2dd9cf717d7836f",
      "fa1fa9f693b4441789e0dafc3928445f",
      "b818a0a2281a4ff482a1c451ca9b2186",
      "130ad286712544da8c25e18c947994d2",
      "6f7cb4f6d7c942f2a8cc0ceaaed16c6e",
      "f08b70158a3d4d828ae70220664d5d05",
      "7d972e96f6bf4beba7c1551cffa62a86",
      "f374644fde234d638f0ab42d36ba1159",
      "32dcc89ef2f04acfa499dd54b11f1908",
      "56d4bea2c7784cbbb987110c47a10ad5",
      "055a6b1a9fbf4326a1d9a9053235d309",
      "c8d7cfbc0a6b41d18e5cc717196abac2",
      "13e83fb216c8421aafba8f8d047e0923",
      "59ae259827be45b8aadfbbe1329848fb",
      "b463f10f113b4dc68682afc4a1775ed5",
      "019d8275061c442e9628f9b67f41938d",
      "b4d7fcf2f63a4e56a5e0a3863d4c244c",
      "cb9d37eab491473f9fdccec1eae2e491",
      "21327dea17a949908f8a5cf39ec573e8",
      "4a51bb5f5e9340d2bc45c44ffa502e26",
      "3bd04740224947db95ac3e8e7ca2fba3",
      "1f70446fa2534a2bb716bb988a7028a1",
      "fbff35a325544b03a821f951b2785827",
      "837fea8bee1f476aaa035be7b0c1943f",
      "fadf71ec8eb841c596db7c6fb29c1dd1",
      "5dbb1671cff945c8b1e7283bedff7f46",
      "296cf5711aed47cf89b66b9e01c33130",
      "9780eacf05f24ef58fb17487c029df7f",
      "d154f00c8a16455f874c2661cbc970c2",
      "88ae2fc1af4b4ffeb28bd077bf1fb6ab",
      "6d6aa34012bb4aa5a8d46f7d33a43915",
      "52f448d866af4be1b448eb21d7ea1ef7",
      "b753f977a0864b35996fc13368ceb9bc",
      "218b7dc00c5b4d959557b253b29499be",
      "2610f886aafb4e2dab8ad3da55c9f35a",
      "f2f018e36e78480486fae0de4bb59901",
      "a6fcc431f699455c9e8062366035484e",
      "dda8d596c372432688a7a19ffa8b3514",
      "c3f7df87d7d0498984ba6adea166ee96",
      "8df1352476654e0aa0c649ee91972412",
      "e967cde0d41f428a8dfc307f89846edf",
      "fca78cd89213438da2d5a20717b3dd59",
      "78985cca9e0a4decab4b009541223065",
      "9daa4146dcdc4e1cb561fd9a9e22afdb",
      "afa4e7a782194887af11e04b25ab226a",
      "9364f903b2ef4cc1868246b2b794445a"
     ]
    },
    "id": "PVKFmqlwCXCB",
    "outputId": "74094ec0-63ca-491c-af2b-287e69f64d8f"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Small, fast slice for first run. Later increase to \"train[:100%]\".\n",
    "raw = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:5000]\", cache_dir=\"/content/.cache_hf\")\n",
    "splits = raw.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds, eval_ds = splits[\"train\"], splits[\"test\"]\n",
    "\n",
    "def format_alpaca(ex):\n",
    "    instr = (ex.get(\"instruction\") or \"\").strip()\n",
    "    inp   = (ex.get(\"input\") or \"\").strip()\n",
    "    out   = (ex.get(\"output\") or \"\").strip()\n",
    "    user  = f\"### Instruction:\\n{instr}\" + (f\"\\n\\n### Input:\\n{inp}\" if inp else \"\")\n",
    "    return {\"text\": f\"{user}\\n\\n### Response:\\n{out}\"}\n",
    "\n",
    "train_data = train_ds.map(format_alpaca, remove_columns=train_ds.column_names)\n",
    "eval_data  = eval_ds.map(format_alpaca,  remove_columns=eval_ds.column_names)\n",
    "\n",
    "print(train_data[0][\"text\"][:500])\n",
    "print(f\"Train/Eval sizes: {len(train_data)}/{len(eval_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6da2db2152ce4fc9aa56adf58cc39fa8",
      "0e275f812e254c1eb5d45bd2b9d74f3e",
      "5844eb81d66f4e47badaee08eb7abc4e",
      "b33dc25fa34144d68dba8c3cf45b7fd4",
      "515ff16b366b4fa79126b0e26b3cffb4",
      "0c32c24c0366486e8368db8673e4fc16",
      "fea3fb29434641728803bdfde375449d",
      "7f7aba690b084463b9d8c17ba1788263",
      "15f65367f9c9478b86cb148d1bac68ff",
      "83e9fbe08c6a4be6b8574694f24763c5",
      "5fe4ca172c12465491da5fbbaa0d7441",
      "58dc6a1388824d58bc69cb3a1cbc425f",
      "e84a65e000d74e60b5f9dbd8e7f01809",
      "beca9b465f404b36ae64393c26baa870",
      "9fd5f7a0ea264ae293a6e947eece0ed9",
      "bf398823d64449ca8f604bc23cd2c594",
      "04a69161f5914feb92089b0c7dfdd132",
      "013ef3ca09a74e12ac1c24666871c44f",
      "50ee5a86beab4869947d4992e58cba5a",
      "d305cd8019a048349ca0db9a2b880bc6",
      "bff79e8b1e324208a4ef653f0dca6075",
      "91f90f3061b145e9b6a72752f1b529a6",
      "009b799e95ac4f66bdf23c283ebbbbf9",
      "61fd4f5b80c74f40a6783548a1424360",
      "191d31bf1a9b48bf907a7493d3f15343",
      "8a4bd40e25e04fedbd579ad28c29147e",
      "ad7ad78d5ee545dd88a6c824f590cd05",
      "04266f1d46374c33a529dd51027eacf3",
      "0042dd2b447f46599e2935c4e31ae5d6",
      "3e922b925e804feda9b4ef906ddbeabe",
      "08c37be8ea0243828db78a6c330e1a5d",
      "5d02a779ba894080987bce2016d5a9f0",
      "9f8dcee6a5704ec0b74d5bdf505158a4",
      "8e45a6e7d1764466bd8ee5df6289b1c7",
      "2f30569a745a45619d84761e2f432f5b",
      "8fbfe9e6e74d4bf4b81a407e49fc5164",
      "95a937b41a7b46acbc4f7b0b945b513f",
      "07b66c0fc58a4c46adfe2bbdacf248a9",
      "9bfa2e9eacba4b82ac3b30c7b729753b",
      "83315af0f5c24653af7f803ddf2ac91b",
      "d461e9211465434caebdce54243e936a",
      "f001f685087b4861b75d2d9d8602660e",
      "e1b4070765ce43a5808524205cc8d949",
      "717a85f6c457407c9d9f271a1c6ef8fe",
      "61cb37edff1a47e6ba4b87f1ac8aa18b",
      "c64223efbf824fa6a7e4b8afa6ec890c",
      "ba77b55b9428474aacf0e79dfb97b7af",
      "2dfe0333d3ca4a66b9023e30ab2962a4",
      "8c9dd89825a548e891963f2632a32eec",
      "e0ef498c759b46f38b08cf81df1e92a7",
      "e3b9732661f843a593f7a3d20bf9474e",
      "bc281522094b4c40b024d3ee81d2b978",
      "d37ca2561ce64ebf8ce7c341ad96e06e",
      "d7a63db3b49945f7804a71b6bde22497",
      "2c2ea857dbc84638917c0d683d8075c5",
      "5c3cd8066b6b498da418aeaea73a7731",
      "927e6cf58dc34d2c9a0a11d8dbc4470a",
      "edeb1d8e26a0417da5da0e6270cc0a90",
      "2280fe0c64d847e0b8420cbcc1b93a20",
      "bc0c01f943b34df1bdbc0a8b28281443",
      "c4f172738be5432095f24a9c622bfdbd",
      "755bca838cc0461b8debecee9a9e60a7",
      "84d6004184b6449fb5171001858f4896",
      "52f925c1828c4736bbd27a3f5da90a26",
      "e80afc55ce7842008d60ca5d301b65ca",
      "20c10a28cc7b41c195f35292e36b63c5",
      "79776a1b11514c6eb77f4c1b8ca24d4a",
      "084e312ed5f040a984e0581b9bf0ba1c",
      "2b05de37b2a94c7fae3291563affa1c4",
      "b2aac4fed21840ac903b88c86690d99a",
      "a26bc4c09cc34e3eb1d8b6541cf851eb",
      "05b1b7caaba242778ce67ba4c2dbc742",
      "c5e4d63607f74bceac73c23da026a4bb",
      "9571491fec2b48edbea999232077eb9d",
      "49179b7ec8034957a1c8b071a60d1539",
      "b0a09257ab4342e0a40e83dfacd8a641",
      "00694a0eb70c4a12b1a563f1e2de9f03",
      "1ea145af3a404d6391e3fc01e1c2dd78",
      "86d1cca9a34e4aeeb077e1618b23d3ce",
      "ab8796965b354162bf2e990a2677456d",
      "a4b25b79de9b4ef1b2ab44aaa197fe98",
      "ed14064de94844c4b34ce192f9efa8f9",
      "0a81e44025f94e81ac8009e9e6313f34",
      "881c435280f847ceae62551375d9ef82",
      "6a9e2c5dbbad45039896beea2d79205a",
      "2411989187c14866b7d3a3c488054318",
      "3a4fd0ed737e4e4b8d4cf6d04353009e",
      "4dbad414f9c445a89ac48c51db60bd8b",
      "ce50972a82c147b1954853d5835190c1",
      "0a4019c286ad41fbae24b3f1f23488b5",
      "4f6675ade4724950a926365f1c3cae97",
      "8c81dcf0c26646838a25d8fb4dd9050f",
      "01e2bc6f86884f809c16e8c7dacee37b",
      "67741cedad0c4fb5a173a178dd709bcc",
      "b7adcced25344003a31266db680cfecf",
      "a6cd496efd9e4ece821b095c9d52de14",
      "cd5b7b05017c4d10bb7707761dd951fb",
      "db80dd88071b4f38ad54c98a2ef59a1d",
      "7ca746e4adaf4c578a5d24860681147b",
      "0f78604fb0c84b0db7531c122ba1c36a",
      "521ad124904944cba96db342ea4bdcc1",
      "e95e73be12324bf394d73f3b7b3b2490",
      "22f8ca19a3a04cdcb17fb6b27bcee60c",
      "e1ff53d134d1476ab3f61c48291e9a54",
      "24f14f27f0b94c0485d25c00780563b3",
      "33e9308caf8a4df0bba90f67136d2590",
      "95293aed14ed495aa1c834a8b61caf78",
      "87495ebc60f14ed99124f3fba8363f22",
      "21469a8dbbe44ba091edbcc8af5c855a",
      "5a31352c3e504a1e9c2e3b9546ddd3eb"
     ]
    },
    "id": "yY3_n-2YCd7A",
    "outputId": "d26101ad-4dab-4d6f-8d96-eec0d5eb4359"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "BASE_MODEL = \"unsloth/smollm2-135m\"\n",
    "MAX_LEN = 1024\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name      = BASE_MODEL,\n",
    "    max_seq_length  = MAX_LEN,\n",
    "    load_in_4bit    = False,     # FULL fine-tune (no quantization)\n",
    "    full_finetuning = True,      # <— key difference vs LoRA\n",
    ")\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"outputs_full\",\n",
    "    per_device_train_batch_size=8,     # small model → can go higher; drop if OOM\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=1,                # or set max_steps=1000 for time-boxed training\n",
    "    logging_steps=20,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    optim=\"adamw_torch\",               # standard AdamW\n",
    "    learning_rate=2e-4,                # small model tolerates a bit higher LR\n",
    "    weight_decay=0.01,\n",
    "    seed=42,\n",
    "    max_seq_length=MAX_LEN,\n",
    "    dataset_text_field=\"text\",\n",
    "    fp16=True,                         # T4 = FP16 (if bf16=False)\n",
    "    bf16=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    args=args,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZcQ0eCKC0EE",
    "outputId": "517dd6c4-7dfb-47ea-ebc7-081df320fd46"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"finetuned_full_smollm2\")\n",
    "tokenizer.save_pretrained(\"finetuned_full_smollm2\")\n",
    "!ls -lah finetuned_full_smollm2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VuWmZZmdpgxi",
    "outputId": "8116d82a-f894-422a-fa6b-c2d93707aa34"
   },
   "outputs": [],
   "source": [
    "# Save the fully fine-tuned model and tokenizer\n",
    "SAVE_DIR = \"full_smollm2_final\"\n",
    "trainer.save_model(SAVE_DIR)           # saves model weights + config\n",
    "tokenizer.save_pretrained(SAVE_DIR)    # saves tokenizer vocab + config\n",
    "print(\"Saved to:\", SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vp4aTdp0Oh6_"
   },
   "source": [
    "# (LoRA) on the same dataset (smaller memory, faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "lL56_hIKDDHl",
    "outputId": "ea1b56e9-9ec2-4eb7-bdac-d1cf922f4cee"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, FastModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "BASE_MODEL = \"unsloth/smollm2-135m\"\n",
    "MAX_LEN = 1024\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name      = BASE_MODEL,\n",
    "    max_seq_length  = MAX_LEN,\n",
    "    load_in_4bit    = True,      # LoRA path (QLoRA)\n",
    "    full_finetuning = False,     # <— enables PEFT\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    max_seq_length=MAX_LEN,\n",
    ")\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"outputs_lora\",\n",
    "    per_device_train_batch_size=8,        # should be fine; lower if OOM\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=20,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    optim=\"adamw_8bit\",                   # 8-bit optimizer\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    seed=42,\n",
    "    max_seq_length=MAX_LEN,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    args=args,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_fIxCoSFQWF",
    "outputId": "154c2b06-2745-47d6-d055-c5fa3540fe65"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"finetuned_lora_smollm2\")\n",
    "tokenizer.save_pretrained(\"finetuned_lora_smollm2\")\n",
    "\n",
    "prompt = \"Write a concise function in Python that reverses a string.\"\n",
    "x = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "with torch.inference_mode():\n",
    "    y = model.generate(**x, max_new_tokens=120, do_sample=True, temperature=0.7)\n",
    "print(tokenizer.decode(y[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kO3Y6ZEUpqd5",
    "outputId": "e1d2712a-1b1a-439c-e9c8-93380b67500f"
   },
   "outputs": [],
   "source": [
    "SAVE_DIR = \"lora_smollm2_final\"\n",
    "trainer.save_model(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "print(\"Saved to:\", SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFsE3ONSNMpl"
   },
   "source": [
    "GRPO (reasoning) with TRL’s GRPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rmi0qYI3NOA5",
    "outputId": "c75e06d4-b356-4456-bc3f-5b879120b63d"
   },
   "outputs": [],
   "source": [
    "!/content/unsloth-venv/bin/python -m pip -q install \"trl==0.24.0\"\n",
    "import os, sys, glob, site\n",
    "venv_path = \"/content/unsloth-venv\"\n",
    "pyver = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "site_dir = f\"{venv_path}/lib/python{pyver}/site-packages\"\n",
    "cands = glob.glob(f\"{venv_path}/lib/python*/site-packages\")\n",
    "if not os.path.isdir(site_dir) and cands: site_dir = cands[0]\n",
    "site.addsitedir(site_dir)\n",
    "\n",
    "import torch, transformers, trl, unsloth\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "print(\"TRL:\", trl.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215,
     "referenced_widgets": [
      "6a52bd7f09a448c8aa0837b7ede32ec0",
      "1bb1179061b64d44a6ced09c83b2e498",
      "a396be0c493a4cdcb979cfc7edaf990e",
      "55076996bc1f457bae15cb39bef123b4",
      "f171c474701e45868850007dc690d9d9",
      "777a41463a8d42708713437aa92b3379",
      "c6f8188ad7bf4f83ae504d4207500e86",
      "b265ec17b36f47deb1a218dd2c2b5623",
      "a30e0176e53c4f65984d27138f93de91",
      "d6b29662f38b410aa5cb77c1421829cb",
      "f273b1e8e37b44208e0069e292c10c07",
      "0ddb6572c98e4aa9bd35c686591cd7de",
      "c89099c95c764e78b33aa71a94850795",
      "3dddcbf1cefc4d0a88fd1d82167b2922",
      "4333bf3e37e14ce2b81c8bdcfd75479c",
      "4ee2e7c4c7a0492594610820c48e8121",
      "264e976ad8cf493db1b4aab4f22f8ab7",
      "65c3fb58292b4217a0e7b5d0d61097b0",
      "ff562c8117da424b976a77b0a84ae30b",
      "768158f1dfda4f1c8b202d7c2d5ddda5",
      "e5ad8023b3bd4c2ba3041524b6f6c4b2",
      "3a3ba9b2085e4e70b031d29bc54b420a",
      "ad6dd575ec3f4793bb56537f4d0aeb81",
      "fda77c8093a7418ab91b385ba6e28904",
      "e78c91f538394e55a7b649fa15a0d6bd",
      "f9b09038758d4e9187cb83caa1d2ff8a",
      "d68cd389a4d0495380e47a89f9683bdd",
      "6b758e00cca249a28063399b37eb3ba0",
      "2a0587f047054e9b89c4f8b3b43e1b97",
      "fd5149bd5d7846b8bbe91b31909211af",
      "bd66788b1a1543f28846028773a869f2",
      "aef3de47bbce4654b9d9ef4a26a36326",
      "acfe986d21b841afa327d481420993ba",
      "7e53cf66165f42b586a360d75676100f",
      "5a695f48f1f04071881dd9b7afab7d5b",
      "6887f3659a1a487483fd17fbeac76ce7",
      "71f2ca3eaaef4e0695bd961af1b43a0b",
      "471caaee966f4640a3489630641510e8",
      "07e87ab3ea774c5498361b0afe8f52ba",
      "0ac8a5d503ad43fb9f10f86286138ff7",
      "e0de30420ba5497ea036e9818635526e",
      "9526f1839ddd4c378b8d484f3e8a0ecf",
      "6d2f8b99a5714833b1ab834070dea3fd",
      "42a220c8668a4c2fa12b83fe81bc19af",
      "e170e3e7e93547a58e707a5ad84bac9e",
      "086161c8fcae4d5aa2ecc3541b980ec8",
      "ae3a9d1dd7aa45afb5f0d2459277ed7f",
      "12e3e013960c445fb983e14402402b55",
      "0857522c209246f2a55aafbcfec5050e",
      "f1e696e955c443199a228a8dd0f47db0",
      "fd86a5d40111477ea08152947f65a0a4",
      "d12f09d75cd24305aba57b51efd2d92d",
      "79a3608f52454e348403a53ee4969792",
      "007c2904d481499eba5af0297fa0b7ad",
      "a462b97b746745a29633dcfe4fd3ac62"
     ]
    },
    "id": "S5OQpewINURG",
    "outputId": "2866a84a-6381-464b-f4e7-adf495e9b6d0"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "gsm = load_dataset(\"openai/gsm8k\", \"main\", split=\"train[:1000]\")  # small subset\n",
    "# Fields: \"question\", \"answer\" (answer often ends with '#### final_number')\n",
    "print(gsm[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26-tzJJONYgg"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_final_number(s: str):\n",
    "    # common GSM8K pattern: \"... #### 42\"\n",
    "    m = re.search(r\"####\\s*(-?\\d+(?:\\.\\d+)?)\", s)\n",
    "    return m.group(1).strip() if m else None\n",
    "\n",
    "def reward_function(samples, **kwargs):\n",
    "    # samples: list[str] model generations\n",
    "    # kwargs can include batch with \"answer\"\n",
    "    rewards = []\n",
    "    golds = kwargs.get(\"references\")  # we’ll pass gold answers separately\n",
    "    for gen, gold in zip(samples, golds):\n",
    "        gold_num = extract_final_number(gold or \"\")\n",
    "        got_num  = extract_final_number(gen or \"\")\n",
    "        rewards.append(1.0 if (gold_num is not None and gold_num == got_num) else 0.0)\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1pXujWANcTx",
    "outputId": "f981a435-73a9-4cb3-e5cd-e49d54cc8d18"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, FastModel\n",
    "\n",
    "BASE_MODEL = \"unsloth/smollm2-135m\"\n",
    "MAX_LEN = 768\n",
    "\n",
    "policy, tokenizer = FastModel.from_pretrained(\n",
    "    model_name      = BASE_MODEL,\n",
    "    max_seq_length  = MAX_LEN,\n",
    "    load_in_4bit    = True,\n",
    "    full_finetuning = False,\n",
    ")\n",
    "policy = FastLanguageModel.get_peft_model(\n",
    "    policy,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    max_seq_length=MAX_LEN,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOyV7RBkNjdf"
   },
   "outputs": [],
   "source": [
    "def to_prompt(ex):\n",
    "    # short, explicit instruction\n",
    "    return f\"\"\"Solve the math word problem step by step, then answer with '#### <final_number>' on the last line.\n",
    "\n",
    "Problem:\n",
    "{ex['question']}\n",
    "\"\"\"\n",
    "\n",
    "prompts = [to_prompt(ex) for ex in gsm]\n",
    "references = [ex[\"answer\"] for ex in gsm]  # used by reward function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mf3aoU2dOpVa",
    "outputId": "7e8cba77-a988-4685-ddce-90f21fd25760"
   },
   "outputs": [],
   "source": [
    "# 1) Build a Dataset with prompts (and keep references separately for the reward)\n",
    "from datasets import Dataset\n",
    "\n",
    "train_ds = Dataset.from_dict({\"prompt\": prompts})\n",
    "len(train_ds), train_ds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rtle0YL8PUMf"
   },
   "outputs": [],
   "source": [
    "# assumes you already created `prompts` and `references` lists earlier\n",
    "prompt_to_answer = {p: a for p, a in zip(prompts, references)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1E7RriuRPXiE"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_final_number(s: str):\n",
    "    m = re.search(r\"####\\s*(-?\\d+(?:\\.\\d+)?)\", s)\n",
    "    return m.group(1).strip() if m else None\n",
    "\n",
    "def reward_function(*, prompts=None, completions=None, completion_ids=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Unsloth GRPO will call this with keyword args. We use:\n",
    "      - prompts:     list[str]   (batch prompts)\n",
    "      - completions: list[str]   (model generations for each prompt)\n",
    "    Return: list[float] rewards length == len(completions)\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for p, gen in zip(prompts or [], completions or []):\n",
    "        gold = prompt_to_answer.get(p, \"\")\n",
    "        got = extract_final_number(gen or \"\")\n",
    "        ans = extract_final_number(gold or \"\")\n",
    "        rewards.append(1.0 if (ans is not None and got == ans) else 0.0)\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQ8did71Sfoq",
    "outputId": "ad1680e2-cf17-468d-d31a-ee8ea8f97e7a"
   },
   "outputs": [],
   "source": [
    "import os, sys, glob, site\n",
    "\n",
    "venv_path = \"/content/unsloth-venv\"\n",
    "pyver = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "\n",
    "# Find the venv site-packages folder automatically\n",
    "site_dir = f\"{venv_path}/lib/python{pyver}/site-packages\"\n",
    "cands = glob.glob(f\"{venv_path}/lib/python*/site-packages\")\n",
    "if not os.path.isdir(site_dir) and cands:\n",
    "    site_dir = cands[0]\n",
    "\n",
    "site.addsitedir(site_dir)\n",
    "\n",
    "import torch, transformers\n",
    "print(\"✅ Attached venv at:\", site_dir)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UOeepWXfTAQy",
    "outputId": "4ed8bbb3-35c8-4bc6-d652-40b636543f1f"
   },
   "outputs": [],
   "source": [
    "# Clean venv\n",
    "!rm -rf /content/unsloth-venv\n",
    "!python -m venv /content/unsloth-venv --without-pip\n",
    "!curl -sS https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
    "!/content/unsloth-venv/bin/python get-pip.py\n",
    "!/content/unsloth-venv/bin/python -m pip install --upgrade pip setuptools wheel\n",
    "\n",
    "# Torch (CUDA 12.4 wheel), Transformers, TRL, and Unsloth + deps\n",
    "!/content/unsloth-venv/bin/python -m pip install --index-url https://download.pytorch.org/whl/cu124 torch==2.5.1+cu124\n",
    "!/content/unsloth-venv/bin/python -m pip install \\\n",
    "  \"transformers==4.56.2\" \"trl==0.24.0\" \\\n",
    "  \"accelerate>=0.34.2\" \"peft>=0.13.2\" \"bitsandbytes>=0.45.0\" \"datasets>=2.20.0\" \\\n",
    "  unsloth unsloth_zoo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGMpUOCKXKZl",
    "outputId": "024280da-e19a-40e2-9a59-ffe5f8ae6119"
   },
   "outputs": [],
   "source": [
    "# Remove conflicting packages from the *venv*\n",
    "!/content/unsloth-venv/bin/python -m pip uninstall -y xformers torchvision\n",
    "\n",
    "# (Optional) make sure torch & core libs are exactly what we want\n",
    "!/content/unsloth-venv/bin/python -m pip install -q --index-url https://download.pytorch.org/whl/cu124 torch==2.5.1+cu124\n",
    "!/content/unsloth-venv/bin/python -m pip install -q \"transformers==4.56.2\" \"trl==0.24.0\" \"accelerate>=0.34.2\" \"peft>=0.13.2\" \"datasets>=2.20.0\" unsloth unsloth_zoo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hSAONzjijChG",
    "outputId": "2a2262b6-2295-46d3-81ce-6cde2b0b420e"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# A small set of arithmetic prompts\n",
    "prompts = [\n",
    "    \"Q: 17 + 28 = ?\\nA:\",\n",
    "    \"Q: 65 - 19 = ?\\nA:\",\n",
    "    \"Q: 7 * 8 = ?\\nA:\",\n",
    "    \"Q: 144 / 12 = ?\\nA:\",\n",
    "    \"Q: 29 + 34 = ?\\nA:\",\n",
    "    \"Q: 81 - 27 = ?\\nA:\",\n",
    "    \"Q: 9 * 9 = ?\\nA:\",\n",
    "    \"Q: 56 / 7 = ?\\nA:\",\n",
    "    \"Q: 12 + 45 = ?\\nA:\",\n",
    "    \"Q: 100 - 58 = ?\\nA:\",\n",
    "]\n",
    "\n",
    "# References (ground-truth answers as strings)\n",
    "references = [\n",
    "    \"45\", \"46\", \"56\", \"12\", \"63\", \"54\", \"81\", \"8\", \"57\", \"42\",\n",
    "]\n",
    "\n",
    "# Trainer expects a dataset with a \"prompt\" field\n",
    "train_ds = Dataset.from_dict({\"prompt\": prompts})\n",
    "len(train_ds), train_ds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZ5pwTHRjINK"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "\n",
    "def extract_number(text):\n",
    "    # keep last number-like token\n",
    "    nums = re.findall(r\"-?\\d+(?:\\.\\d+)?\", text)\n",
    "    return nums[-1] if nums else None\n",
    "\n",
    "@torch.no_grad()\n",
    "def reward_math(prompts, completions, completion_ids=None, **kwargs):\n",
    "    refs = kwargs.get(\"references\", [])\n",
    "    rewards = []\n",
    "    for i, (p, c) in enumerate(zip(prompts, completions)):\n",
    "        ref = refs[i % len(refs)]  # cycle if needed\n",
    "        pred = extract_number(c) or \"\"\n",
    "        # exact match reward; partial credit if digits overlap\n",
    "        if pred.strip() == ref.strip():\n",
    "            r = 1.0\n",
    "        elif re.sub(r\"\\D\", \"\", pred) == re.sub(r\"\\D\", \"\", ref) and pred != \"\":\n",
    "            r = 0.5\n",
    "        else:\n",
    "            r = 0.0\n",
    "        rewards.append(r)\n",
    "    return torch.tensor(rewards, device=\"cuda\", dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ysTpUPtLjnHg",
    "outputId": "bef93ab0-7948-497a-988e-b4652e9d8373"
   },
   "outputs": [],
   "source": [
    "print(\"🚀 Starting GRPO reasoning training…\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AUzAL5Gyj4Og"
   },
   "outputs": [],
   "source": [
    "def generate(model, tok, prompt):\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        max_new_tokens=64,\n",
    "        pad_token_id=tok.eos_token_id,\n",
    "    )\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "test_q = \"Q: 23 + 19 = ?\\nA:\"\n",
    "print(\"=== After GRPO ===\")\n",
    "print(generate(trainer.model.eval(), tokenizer, test_q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KyOcG4UPbAbD",
    "outputId": "733869e8-b95c-41d6-ffdf-20a4c5df3d80"
   },
   "outputs": [],
   "source": [
    "# should print a GPU name and True\n",
    "import torch, platform\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA build:\", torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11f8Nzn5ONr9"
   },
   "source": [
    "# (GRPO Reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9C3nb4AlPAS",
    "outputId": "f664f081-f851-4c86-f702-45f869ea9f93"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(\"✅ CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66EEa_4YlW4P",
    "outputId": "a9998a4d-0d3c-4a7c-ac9a-077e6a235576"
   },
   "outputs": [],
   "source": [
    "!pip install -q \"transformers==4.57.1\" \"trl==0.24.0\" \\\n",
    "               \"accelerate>=0.34.2\" \"peft>=0.13.2\" \\\n",
    "               \"datasets>=2.20.0\" \"bitsandbytes>=0.45.0\" \\\n",
    "               unsloth unsloth_zoo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k5kNtZlfloDf"
   },
   "outputs": [],
   "source": [
    "# === 1. Dataset ===\n",
    "from datasets import load_dataset, Dataset\n",
    "import re\n",
    "\n",
    "gsm = load_dataset(\"openai/gsm8k\", \"main\", split=\"train[:1000]\")\n",
    "\n",
    "def to_prompt(rec):\n",
    "    return f\"\"\"Solve the math word problem step by step, then answer with '#### <final_number>' on the last line.\n",
    "\n",
    "Problem:\n",
    "{rec['question']}\n",
    "\"\"\"\n",
    "prompts = [to_prompt(x) for x in gsm]\n",
    "answers = [x[\"answer\"] for x in gsm]\n",
    "train_ds = Dataset.from_dict({\"prompt\": prompts})\n",
    "answer_map = dict(zip(prompts, answers))\n",
    "\n",
    "# === 2. Reward function ===\n",
    "def extract_final_number(text):\n",
    "    m = re.search(r\"####\\s*(-?\\d+(?:\\.\\d+)?)\", text)\n",
    "    return m.group(1).strip() if m else None\n",
    "\n",
    "def reward_fn(*, prompts=None, completions=None, completion_ids=None, **kwargs):\n",
    "    rewards = []\n",
    "    for p, gen in zip(prompts or [], completions or []):\n",
    "        gold = answer_map.get(p, \"\")\n",
    "        got  = extract_final_number(gen or \"\")\n",
    "        ref  = extract_final_number(gold or \"\")\n",
    "        rewards.append(1.0 if got == ref and ref is not None else 0.0)\n",
    "    return rewards\n",
    "\n",
    "# === 3. Model (LoRA + 4-bit) ===\n",
    "from unsloth import FastLanguageModel, FastModel\n",
    "\n",
    "BASE_MODEL = \"unsloth/smollm2-135m\"\n",
    "MAX_LEN    = 768\n",
    "\n",
    "policy, tokenizer = FastModel.from_pretrained(\n",
    "    model_name      = BASE_MODEL,\n",
    "    max_seq_length  = MAX_LEN,\n",
    "    load_in_4bit    = True,\n",
    "    full_finetuning = False,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "policy = FastLanguageModel.get_peft_model(\n",
    "    policy,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "                    \"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    max_seq_length=MAX_LEN,\n",
    ")\n",
    "\n",
    "# === 4. GRPO training ===\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "\n",
    "cfg = GRPOConfig(\n",
    "    output_dir=\"outputs_grpo\",\n",
    "    per_device_train_batch_size=4,   # raise to 8 on L4/A100\n",
    "    gradient_accumulation_steps=1,\n",
    "    max_steps=100,                   # longer = better; start small\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    learning_rate=5e-6,\n",
    "    max_prompt_length=384,\n",
    "    max_completion_length=192,\n",
    "    beta=0.03,\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=policy,\n",
    "    tokenizer=tokenizer,\n",
    "    args=cfg,\n",
    "    reward_funcs=[reward_fn],\n",
    "    train_dataset=train_ds,\n",
    "    train_kwargs={},\n",
    ")\n",
    "\n",
    "print(\"✅ Starting GRPO training… (first step compiles kernels, be patient)\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-_AqfcJq8Dq",
    "outputId": "c7355af1-86b3-4016-8a19-0fb645bcf8b5"
   },
   "outputs": [],
   "source": [
    "# 1) Build policy (LoRA) and frozen ref from SAME BASE\n",
    "from unsloth import FastLanguageModel, FastModel\n",
    "\n",
    "BASE_MODEL = \"unsloth/smollm2-135m\"\n",
    "MAX_LEN    = 768\n",
    "\n",
    "policy, tokenizer = FastModel.from_pretrained(BASE_MODEL, max_seq_length=MAX_LEN, load_in_4bit=True, full_finetuning=False)\n",
    "ref_policy, _     = FastModel.from_pretrained(BASE_MODEL, max_seq_length=MAX_LEN, load_in_4bit=True, full_finetuning=False)\n",
    "\n",
    "# pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# add LoRA\n",
    "policy = FastLanguageModel.get_peft_model(\n",
    "    policy,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    max_seq_length=MAX_LEN,\n",
    ")\n",
    "\n",
    "# 2) Make BOTH models return hidden states; freeze ref; disable cache for training\n",
    "for m in (policy, ref_policy):\n",
    "    m.config.output_hidden_states = True\n",
    "    m.config.use_cache = False\n",
    "policy.train(); ref_policy.eval()\n",
    "for p in ref_policy.parameters():\n",
    "    p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxI6jR8usUQ_"
   },
   "outputs": [],
   "source": [
    "# 3) GRPO config with explicit num_generations (batch size must be a multiple)\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "\n",
    "NUM_GENERATIONS = 2       # try 2 on T4 to reduce memory\n",
    "PER_DEVICE_BSZ  = 4       # MUST be 2, 4, 6… (multiple of NUM_GENERATIONS)\n",
    "\n",
    "cfg = GRPOConfig(\n",
    "    output_dir=\"outputs_grpo_unsloth\",\n",
    "    per_device_train_batch_size=PER_DEVICE_BSZ,\n",
    "    gradient_accumulation_steps=1,\n",
    "    max_steps=100,\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    learning_rate=5e-6,\n",
    "    max_prompt_length=320,\n",
    "    max_completion_length=128,\n",
    "    beta=0.03,\n",
    "    num_generations=NUM_GENERATIONS,\n",
    "    dataloader_num_workers=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Twe-rI1duH9f",
    "outputId": "56a59a86-0a16-4364-ccc2-b37db7122a82"
   },
   "outputs": [],
   "source": [
    "# Must be set BEFORE importing unsloth\n",
    "import os\n",
    "os.environ[\"UNSLOTH_DONT_PATCH_TRAINERS\"] = \"1\"\n",
    "\n",
    "# Import TRL/Transformers/Torch FIRST\n",
    "import torch, transformers, trl\n",
    "\n",
    "# Grab stable references to vanilla TRL classes NOW\n",
    "VanillaGRPOTrainer = trl.GRPOTrainer\n",
    "VanillaGRPOConfig  = trl.GRPOConfig\n",
    "\n",
    "print(\"Vanilla GRPOTrainer from:\", VanillaGRPOTrainer.__module__)\n",
    "\n",
    "# Now import Unsloth (model speedups), AFTER we saved the vanilla classes\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, FastModel\n",
    "\n",
    "print(\"Torch:\", torch.__version__, \"| Transformers:\", transformers.__version__, \"| TRL:\", trl.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHkf4ioFvOjR"
   },
   "outputs": [],
   "source": [
    "!pip -q install \"transformers==4.57.1\" \"trl==0.24.0\" \\\n",
    "                \"accelerate>=0.34.2\" \"peft>=0.13.2\" \\\n",
    "                \"datasets>=2.20.0\" \"bitsandbytes>=0.45.0\" \\\n",
    "                unsloth unsloth_zoo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8EFRGXzmvTji",
    "outputId": "e3a0b614-3b85-49bd-b711-24a7874a5a4f"
   },
   "outputs": [],
   "source": [
    "# MUST be set before importing unsloth\n",
    "%env UNSLOTH_DONT_PATCH_TRAINERS=1\n",
    "import os\n",
    "os.environ[\"UNSLOTH_DONT_PATCH_TRAINERS\"] = \"1\"\n",
    "\n",
    "# Import TRL first, and take references to its vanilla GRPO classes\n",
    "import torch, transformers, trl\n",
    "VanillaGRPOTrainer = trl.GRPOTrainer\n",
    "VanillaGRPOConfig  = trl.GRPOConfig\n",
    "print(\"Vanilla GRPOTrainer from:\", VanillaGRPOTrainer.__module__)\n",
    "\n",
    "# Now import Unsloth (model speedups); it will NOT patch trainers\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, FastModel\n",
    "\n",
    "print(\"Torch:\", torch.__version__, \"| Transformers:\", transformers.__version__, \"| TRL:\", trl.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9RuDd4OwJyN"
   },
   "outputs": [],
   "source": [
    "# Data + reward (same as before)\n",
    "from datasets import load_dataset, Dataset\n",
    "import re\n",
    "\n",
    "gsm = load_dataset(\"openai/gsm8k\", \"main\", split=\"train[:1000]\")\n",
    "def to_prompt(rec):\n",
    "    return f\"\"\"Solve the math word problem step by step, then answer with '#### <final_number>' on the last line.\n",
    "\n",
    "Problem:\n",
    "{rec['question']}\n",
    "\"\"\"\n",
    "prompts    = [to_prompt(x) for x in gsm]\n",
    "references = [x[\"answer\"] for x in gsm]\n",
    "train_ds   = Dataset.from_dict({\"prompt\": prompts})\n",
    "prompt2gold = dict(zip(prompts, references))\n",
    "\n",
    "def extract_final_number(s: str):\n",
    "    m = re.search(r\"####\\s*(-?\\d+(?:\\.\\d+)?)\", s)\n",
    "    return m.group(1).strip() if m else None\n",
    "\n",
    "def reward_fn(*, prompts=None, completions=None, **kwargs):\n",
    "    rewards = []\n",
    "    for p, gen in zip(prompts or [], completions or []):\n",
    "        gold = prompt2gold.get(p, \"\")\n",
    "        got  = extract_final_number(gen or \"\")\n",
    "        ans  = extract_final_number(gold or \"\")\n",
    "        rewards.append(1.0 if (ans is not None and got == ans) else 0.0)\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UkhQEOocwP4l",
    "outputId": "91b67974-cb33-4a7f-b1e2-ac6fbd594874"
   },
   "outputs": [],
   "source": [
    "# Unsloth model (4-bit + LoRA)\n",
    "BASE_MODEL = \"unsloth/smollm2-135m\"\n",
    "MAX_LEN    = 768\n",
    "\n",
    "policy, tokenizer = FastModel.from_pretrained(\n",
    "    model_name      = BASE_MODEL,\n",
    "    max_seq_length  = MAX_LEN,\n",
    "    load_in_4bit    = True,\n",
    "    full_finetuning = False,\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "policy = FastLanguageModel.get_peft_model(\n",
    "    policy,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    max_seq_length=MAX_LEN,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S-k1mhONyIBx",
    "outputId": "7f41708b-cba7-416e-b184-358bbb21aa85"
   },
   "outputs": [],
   "source": [
    "# VANILLA TRL GRPO config — T4 safe & divisible\n",
    "cfg = VanillaGRPOConfig(\n",
    "    output_dir=\"outputs_grpo_vanilla\",\n",
    "    per_device_train_batch_size=2,   # small to fit T4\n",
    "    generation_batch_size=2,         # <-- divisible by num_generations\n",
    "    num_generations=2,               # <-- lower from default 8\n",
    "\n",
    "    gradient_accumulation_steps=1,\n",
    "    max_steps=100,\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    learning_rate=5e-6,\n",
    "\n",
    "    # keep lengths modest for memory\n",
    "    max_prompt_length=320,\n",
    "    max_completion_length=128,\n",
    "\n",
    "    beta=0.03,\n",
    "    dataloader_num_workers=0,\n",
    "\n",
    "    # precision for T4\n",
    "    bf16=False,\n",
    "    fp16=True,\n",
    "    tf32=False,\n",
    ")\n",
    "\n",
    "# ✅ Correct for trl==0.24.0 (no tokenizer arg)\n",
    "trainer = VanillaGRPOTrainer(\n",
    "    model=policy,\n",
    "    args=cfg,\n",
    "    reward_funcs=[reward_fn],\n",
    "    train_dataset=train_ds,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"✅ Starting GRPO (vanilla TRL with Unsloth model)…\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qosmtuXl6WRd",
    "outputId": "16a004e4-4b88-411c-a5db-00841b8b06ec"
   },
   "outputs": [],
   "source": [
    "print(\"✅ Starting GRPO (vanilla TRL with Unsloth model)…\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5f9e4a22e7aa496298142f729d0a42be",
      "dab3c3a43af941648a8e9dbde7ed1b9c",
      "7cd57718521342bc926f6d544537c9eb",
      "8bd0c5474a524eaa93faea95b990d677",
      "79f226c3f3b84e9ea3484be8ed73bc4d",
      "25e8a22462fd4591ad3471a07dc8a509",
      "38034e77644e423c9e7298302a5e7135",
      "4cc04963c40e44df950fb24a645d9e21",
      "d3fc1b57376544f5b6d0ed041b92b3b0",
      "f6b36de4a1ba44e0b59a87b099b5391c",
      "49d804f25a7542d3b3153dff46df6f89"
     ]
    },
    "id": "tHtcFJwPIpMp",
    "outputId": "45ee3f13-184f-4f55-d52b-b5975f5a5ecf"
   },
   "outputs": [],
   "source": [
    "# ⚡ Fast, batched evaluation on a smaller slice\n",
    "from datasets import load_dataset\n",
    "from math import ceil\n",
    "from tqdm.auto import tqdm\n",
    "import torch, re, time\n",
    "\n",
    "# --- knobs you can tweak ---\n",
    "N_TEST = 80          # try 40–100; smaller = faster\n",
    "BATCH  = 8           # 4–16 depending on VRAM\n",
    "MAX_PROMPT_LEN = 320\n",
    "MAX_NEW = 96         # shorter = faster; 64–128 is fine\n",
    "USE_FP16 = True      # keep True on T4\n",
    "\n",
    "# --- helpers ---\n",
    "def to_prompt(rec):\n",
    "    return f\"\"\"Solve the math word problem step by step, then answer with '#### <final_number>' on the last line.\n",
    "\n",
    "Problem:\n",
    "{rec['question']}\n",
    "\"\"\"\n",
    "\n",
    "def extract_final_number(s: str):\n",
    "    m = re.search(r\"####\\s*(-?\\d+(?:\\.\\d+)?)\", s)\n",
    "    return m.group(1).strip() if m else None\n",
    "\n",
    "# --- data ---\n",
    "test = load_dataset(\"openai/gsm8k\", \"main\", split=f\"test[:{N_TEST}]\")\n",
    "prompts_eval = [to_prompt(x) for x in test]\n",
    "answers_eval = [x[\"answer\"] for x in test]\n",
    "\n",
    "policy.eval()\n",
    "device = next(policy.parameters()).device\n",
    "\n",
    "# batch tokenization helper (pad to same length to keep it fast)\n",
    "def batch_encode(texts):\n",
    "    return tokenizer(\n",
    "        texts, return_tensors=\"pt\", padding=True, truncation=True,\n",
    "        max_length=MAX_PROMPT_LEN\n",
    "    ).to(device)\n",
    "\n",
    "correct = 0\n",
    "gens = []\n",
    "t0 = time.time()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    if USE_FP16 and torch.cuda.is_available():\n",
    "        autocast_ctx = torch.cuda.amp.autocast(dtype=torch.float16)\n",
    "    else:\n",
    "        # No-op context manager\n",
    "        class _noop:\n",
    "            def __enter__(self): return None\n",
    "            def __exit__(self, *a): return False\n",
    "        autocast_ctx = _noop()\n",
    "\n",
    "    with autocast_ctx:\n",
    "        for i in tqdm(range(0, len(prompts_eval), BATCH), desc=\"Evaluating\"):\n",
    "            batch_prompts = prompts_eval[i:i+BATCH]\n",
    "            batch_answers = answers_eval[i:i+BATCH]\n",
    "\n",
    "            x = batch_encode(batch_prompts)\n",
    "\n",
    "            # greedy decoding is fastest & stable for exact-match metric\n",
    "            y = policy.generate(\n",
    "                **x,\n",
    "                max_new_tokens=MAX_NEW,\n",
    "                do_sample=False,\n",
    "                num_beams=1,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "            outs = tokenizer.batch_decode(y, skip_special_tokens=True)\n",
    "            gens.extend(outs)\n",
    "\n",
    "            for out, gold in zip(outs, batch_answers):\n",
    "                got = extract_final_number(out)\n",
    "                ref = extract_final_number(gold)\n",
    "                if got is not None and got == ref:\n",
    "                    correct += 1\n",
    "\n",
    "dt = time.time() - t0\n",
    "acc = correct / len(prompts_eval)\n",
    "print(f\"\\n✅ Exact final-number accuracy: {acc:.3f} on {len(prompts_eval)} problems\")\n",
    "print(f\"⏱️ Time: {dt:.1f}s | Throughput: {len(prompts_eval)/dt:.2f} samples/sec\")\n",
    "\n",
    "# peek at a few generations\n",
    "for k in range(min(3, len(gens))):\n",
    "    print(f\"\\n--- Example {k+1} ---\")\n",
    "    print(gens[k][:800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "id": "_ntYR6m_J1c-",
    "outputId": "7953d995-a710-4144-8f8c-fe6a8c35f1e3"
   },
   "outputs": [],
   "source": [
    "import re, pandas as pd, os\n",
    "\n",
    "def extract_final_number(s: str):\n",
    "    m = re.search(r\"####\\s*(-?\\d+(?:\\.\\d+)?)\", s)\n",
    "    return m.group(1).strip() if m else None\n",
    "\n",
    "rows = []\n",
    "for p, gold, gen in zip(prompts_eval, answers_eval, gens):\n",
    "    pred = extract_final_number(gen or \"\")\n",
    "    ref  = extract_final_number(gold or \"\")\n",
    "    rows.append({\n",
    "        \"prompt\": p,\n",
    "        \"gold_answer\": gold,\n",
    "        \"generated\": gen,\n",
    "        \"pred_final_number\": pred,\n",
    "        \"ref_final_number\": ref,\n",
    "        \"correct\": (pred is not None and ref is not None and pred == ref),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "csv_path = \"grpo_eval_results.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(\"Saved:\", os.path.abspath(csv_path))\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LgrfDE1yKNvO",
    "outputId": "53201b9b-4a4d-439b-d2fb-dc1633b0ad5e"
   },
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "\n",
    "artifacts = [\n",
    "    \"grpo_smollm2_lora_adapters\",  # comment out if you didn't save adapters\n",
    "    \"grpo_smollm2_merged_fp16\",    # comment out if you didn't merge\n",
    "    \"outputs_grpo_vanilla\",        # trainer logs/checkpoints\n",
    "    \"grpo_eval_results.csv\",\n",
    "]\n",
    "\n",
    "for a in artifacts:\n",
    "    if os.path.exists(a):\n",
    "        out = a.rstrip(\"/\").replace(\"/\", \"_\") + \".zip\"\n",
    "        shutil.make_archive(a, 'zip', a) if os.path.isdir(a) else shutil.copy(a, out)\n",
    "        print(\"Zipped:\", out)\n",
    "    else:\n",
    "        print(\"Skip (not found):\", a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHWRokz8qEqY",
    "outputId": "573d44be-a003-48f9-c8e7-98f5d1320447"
   },
   "outputs": [],
   "source": [
    "SAVE_DIR = \"grpo_smollm2_final\"\n",
    "trainer.save_model(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "print(\"Saved to:\", SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUspALpDN4aL"
   },
   "source": [
    "# Continued Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h7I9hAavwi3E",
    "outputId": "bd33555d-c768-4320-a611-f78313e37ee0"
   },
   "outputs": [],
   "source": [
    "!pip -q install \"transformers==4.57.1\" \"peft==0.13.2\" \\\n",
    "                \"datasets>=2.20.0\" \"accelerate>=0.34.2\" \\\n",
    "                \"bitsandbytes>=0.45.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GX3uE7Z_vn6G",
    "outputId": "bc21981b-432e-4367-c05e-2b6a05a93d52"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import unsloth\n",
    "    raise RuntimeError(\"unsloth is present (should NOT be). Do: Runtime → Factory reset runtime, then rerun installs.\")\n",
    "except Exception as e:\n",
    "    print(\"✅ unsloth not importable (good):\", type(e).__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "86e99b943e804f0a8440186360767ebc",
      "44aba15247584c81b2d289d77a8d03ad",
      "38344104399d4c4eb135ed9780685e19",
      "e2717cbc8ef14a30be06841f6d81a38d",
      "6a9e24bb04ba497ab33c86adb04757f4",
      "115d5f108e9043168bf46aad685e9b18",
      "2f99c4712f854f82badd9072d4070729",
      "435263a3791e4a3ea8c1bcb36b328565",
      "4ce589d6bc994bb2a25536eaf5e466c8",
      "c294273824ea4b66a3dd5c4f80ff92ce",
      "d25e38b4ee93493aa72bfd09c84d27b5"
     ]
    },
    "id": "HTxP5QN50aFP",
    "outputId": "4436d600-1652-4bc5-dab4-0a17b709faf4"
   },
   "outputs": [],
   "source": [
    "# ✅ Build Hindi text dataset (streamed) → train_ds / valid_ds\n",
    "from datasets import load_dataset, Dataset\n",
    "from itertools import islice\n",
    "\n",
    "N_LINES = 8000   # lower to 6000 if you want it faster\n",
    "\n",
    "def get_hi_stream():\n",
    "    # 1) Parquet-backed Wikipedia from the wikimedia org (no custom code)\n",
    "    for snap in [\"20231101.hi\", \"20240101.hi\"]:\n",
    "        try:\n",
    "            print(f\"Trying wikimedia/wikipedia:{snap} (parquet)…\")\n",
    "            return load_dataset(\"wikimedia/wikipedia\", snap, split=\"train\", streaming=True)\n",
    "        except Exception as e:\n",
    "            print(\"  -> failed:\", type(e).__name__)\n",
    "    # 2) Classic wikipedia loader (requires trust_remote_code)\n",
    "    for snap in [\"20231101.hi\", \"20230601.hi\"]:\n",
    "        try:\n",
    "            print(f\"Trying wikipedia:{snap} with trust_remote_code…\")\n",
    "            return load_dataset(\"wikipedia\", snap, split=\"train\", streaming=True, trust_remote_code=True)\n",
    "        except Exception as e:\n",
    "            print(\"  -> failed:\", type(e).__name__)\n",
    "    raise RuntimeError(\"No public Hindi Wikipedia snapshot available in this runtime.\")\n",
    "\n",
    "stream = get_hi_stream()\n",
    "\n",
    "def cleaned_lines(gen):\n",
    "    for row in gen:\n",
    "        txt = (row.get(\"text\") or \"\").strip()\n",
    "        if len(txt) > 10:\n",
    "            yield txt\n",
    "\n",
    "sample = list(islice(cleaned_lines(stream), N_LINES))\n",
    "if len(sample) < 1000:\n",
    "    raise RuntimeError(f\"Only {len(sample)} lines collected — increase N_LINES or re-run.\")\n",
    "\n",
    "cut = int(len(sample) * 0.98)\n",
    "train_ds = Dataset.from_dict({\"text\": sample[:cut]})\n",
    "valid_ds = Dataset.from_dict({\"text\": sample[cut:]})\n",
    "\n",
    "print(f\"✅ Dataset ready: {len(train_ds)} train / {len(valid_ds)} valid\")\n",
    "print(\"• Example:\", train_ds[0][\"text\"][:140].replace(\"\\n\",\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323,
     "referenced_widgets": [
      "48302ee96ac6486e8a6046c7c5d0b5d7",
      "9270ecbc26084fe69e240ddb03a557e0",
      "1c392736b7a249c4b9b091b4c6d49b06",
      "a9e940c6ef64489bae79cb48641c8c7c",
      "64684ae2b5f740118a9867bcef3bdaf6",
      "9f6b82448d834e8a8ab796a6d471fa3c",
      "77210a27b9ab42c0aac5a66a2cede7aa",
      "98ab648b90e743f0855ad656d768ed60",
      "cf6e5af91be44837875013a78f23bcf4",
      "c892480ea5ea41a8943341f3c7eda1c4",
      "1012c60971dd404ab15f3726b5b2bc8f",
      "691dd800ea73446380306f77dc5a3a96",
      "750bfd565f1c4008acb528a03dfbe01c",
      "04bcecf0d4304fedb35a34200f700d76",
      "4b5e3f6600bc4218bc0307c64b3bf4a2",
      "700bbfd2285c43749f4f1303cb5a2917",
      "3e5963eaf5e54112bb299f7f9f289a10",
      "dd3866bef732487895f37da92fc4b496",
      "4120d8499ba448efa58d93eeba98e623",
      "9112e9cdb65646258aa0893e8fa535c7",
      "fc74b8ea6fe6423f8b53be2521d677e7",
      "d3183137e3094380bbe47e8437a527cb",
      "78bf520588214a6fae298c1725d912aa",
      "a01d85d97cce4a4880a341c96a9ad3b2",
      "8bc04c1ce03345d4b539ca9c320aca1f",
      "4c67644b0261450da6ebef35126eb3c9",
      "debe1a6bf2ad4410b0317dc1e89ee437",
      "bbdb2a3a87084039b2cfba38e9967d62",
      "128d4fd4eade47e4bd3b8c0dee5be868",
      "bb7404f4ffa043e1a09831410fdb8c35",
      "c1e01b22da3241209ea648e41cbf00bf",
      "df391d7fd0284ffb91913cc2211e4f0a",
      "4380b7af7d7b4c75ab7ce9e8efa7bc79",
      "818cb9d46db245b38eaef6a5104fb640",
      "9bd33ca273ea4dd3bb1c20415e192663",
      "de1b96854714409582d54fa7c7b4f77f",
      "44dbbfbfe9f64053adcd18bac3a506e7",
      "d920ecfb2cbd4c4d8ffb272efee43eef",
      "689214b7b54a4be2a07252bf466ec0e6",
      "8b5c20e980c44417ad0358af6e5a68b6",
      "b42d4c92de494c34b0ff09b520a4fc1c",
      "7384cf814a944a2db25a83cbaa7408fb",
      "b0c8589697c74417943a1f85bf0dbed7",
      "cc066cbc75a64006b384a23d026d088b",
      "2f7a3ea322814777a769688c624ab204",
      "e625ade13f6d4c149ed6cf11e73b8044",
      "b771928e6e984425bd3b7c14967476e8",
      "6b4fb65b90a246c494fc26f091751db3",
      "6bb80c47019f4b7ab4b2ad245ec3a43c",
      "3e3cf586127d4346b6ce163ba382a292",
      "3b001cf3e0f74ba0a85e0e89ce33c50a",
      "249a672fde1341df9651c1eef5e46b7b",
      "30e732ae35624f9d9ce71a8cd734fbcf",
      "9e0ed5c75c54492990ad04ac506aad48",
      "de6b35eb31d144dda310f4350573a6cb",
      "b7d25abeae6a4437b6c9150014b9ca3f",
      "a3328aee5eee4daeb6414475c7461337",
      "6a5be495e47744718b7018627a7726c9",
      "bbbc3f91762d43d2b845464d7eeb74d5",
      "0c27647166f14e1795ab1ebf4c18d24a",
      "78071e619b7c4110acc5a994f27733e1",
      "fd720be4abf94291bb5c253e259d17ec",
      "39b8062816774968b0c552b65b84df06",
      "aaa6cf7705414e93b8c4fc99a5879ab0",
      "07c6d3172495421f992819d12ee73814",
      "9d158902b935446d923c086722d34e4c",
      "49cfd642f52a4281891d61c02a471c86",
      "e222d8b17c684f67ab29cca0afdd2108",
      "cd2aebd1e6704edfba7a644b073ad939",
      "179e0339fbef4c519af5d01502df6d1f",
      "81fee3967c154863a3627d2e06b79388",
      "5b510d0e9cd64e54a7bc13a7cb8c09f0",
      "85f870a5c0da41e4914bf0438a4bc015",
      "d33117a891de48e5a253690b00048b92",
      "7f90203fd8e0494ca43409782c087de5",
      "337a53083bda473ab43fd4549593a067",
      "57dcdaa7aba544c48ffd3bcc01b864aa",
      "82c17510f3744c029bbf4d67f4c14009",
      "7862e6d8096941138d8dfe362f07ae89",
      "372bad61587d40c588443203cfb994d2",
      "77d7bae072034246b0da672c694c8144",
      "a2344eb0761340b68bbf131a25b10393",
      "f82f42538fe84b6d9f1433100ee0f22d",
      "353358f270724e8c9cbf62900948ec97",
      "06ad820946ae44708871b639df900d00",
      "5a2538d32f7848d2a25c79f701fe59f0",
      "6ddcec8451cc41ccb28c6685e392bf0d",
      "fbb74bed5af740049d2e823ccb0bea59",
      "069c03891796476d9dd93e7e0ce29bfc",
      "3488b58f3b984fd9838ba52efdca35ab",
      "091810cead9e469cac38f2c8a3576faf",
      "09d475642cec486caa2f1d58fa8fa0ca",
      "67ef8307f119458095c2500d4b46f3d6",
      "cf138e5ab4374d9aa9605723c7662d89",
      "46dcdf78f14c4339a1bd94c99b5a3a2b",
      "9418add0f25941108f1c5d5cb002881f",
      "728fe17ebd064c1e94930fb4500d619c",
      "5a30e052dfc044b99669103cd809182c",
      "397c331bf75d4c329c7e37f1ac0ccb24"
     ]
    },
    "id": "HcjQ6ISO0hj4",
    "outputId": "3efde437-16a7-4110-f5fb-b6897fd2a2a3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "BASE_MODEL = \"unsloth/smollm2-135m\"\n",
    "MAX_LEN    = 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.model_max_length = MAX_LEN\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL, quantization_config=bnb_cfg, device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ea1d3177a01149c1ae43ae683508fd01",
      "929a837ba6134a999c57b9c9a00c8199",
      "61080facd72143b389cebd634fbce47e",
      "2e3f80bbcc0546208392a7879203cb32",
      "dd4ab3712494499ea3382cee0293b5bf",
      "bc1a9fb13eb14459aa9e31133c58a517",
      "29b0e756277946768f733cf4d53ec079",
      "f3819d3edd92458c97e3bfedcbda9432",
      "089c2cec64c948d785e0f76e6d2ccb97",
      "18ecbb9e0ff14648859db54e122fd5aa",
      "83a8cd7314364909ba497aabb7a9fdde",
      "bc49fb63db274e5f990ce77bab697f56",
      "c05640a512544ea6aed598b33bc5b62c",
      "fb7daa9be34a41c9992b745f621b9d07",
      "0274da76f0a94cb0af9fd783ac263e7b",
      "283e799651d946ea9121f4d1b3b8272f",
      "c4af3a74f7514037895797761adb8c19",
      "492ec04558864320be0a92f4c42fbcca",
      "6a029a6a9876482fabe55b9c8ef6805b",
      "0d31046791214b3ca10a7d697b65e576",
      "d4f252b614b84075aa80acb7c2bce517",
      "bd146a2a1d854e6c9fce26cc5b83113e"
     ]
    },
    "id": "ERA92euJ07Nc",
    "outputId": "11291090-192a-41e8-964a-a708a7aa8f41"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "def tok_fn(batch):\n",
    "    out = tokenizer(\n",
    "        batch[\"text\"], truncation=True, max_length=MAX_LEN, padding=\"max_length\",\n",
    "    )\n",
    "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "    return out\n",
    "\n",
    "cols = list(train_ds.column_names)\n",
    "t_train = train_ds.map(tok_fn, batched=True, remove_columns=cols)\n",
    "t_eval  = valid_ds.map(tok_fn, batched=True, remove_columns=cols)\n",
    "t_train.set_format(\"torch\"); t_eval.set_format(\"torch\")\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"outputs_cpt_hi_hf\",\n",
    "    per_device_train_batch_size=4,     # if OOM: 2\n",
    "    gradient_accumulation_steps=2,     # effective batch ~8\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4, warmup_ratio=0.03, weight_decay=0.0,\n",
    "    logging_steps=20, save_steps=200, save_total_limit=2,\n",
    "    fp16=True, bf16=False, tf32=False, report_to=[], dataloader_num_workers=0,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=args,\n",
    "    train_dataset=t_train, eval_dataset=t_eval,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "print(\"✅ Starting Continued Pretraining (Hindi)…\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "sISp6pmbDUNg",
    "outputId": "f8d7d813-806a-4a43-f80b-269c673f8f17"
   },
   "outputs": [],
   "source": [
    "import math, os\n",
    "metrics = trainer.evaluate()\n",
    "loss = metrics.get(\"eval_loss\", None)\n",
    "ppl = math.exp(loss) if loss is not None else None\n",
    "print(f\"Validation loss: {loss} | Perplexity: {ppl}\")\n",
    "\n",
    "out_dir = \"cpt_hi_smollm2_lora_adapters\"\n",
    "model.save_pretrained(out_dir)\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "print(\"💾 Saved adapters to:\", os.path.abspath(out_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MtmZb_hGDa-R",
    "outputId": "3b000dcd-bf56-40d1-8162-1476ba80d10a"
   },
   "outputs": [],
   "source": [
    "prompt = \"भारत की स्वतंत्रता के इतिहास पर तीन वाक्य लिखिए।\"\n",
    "x = tokenizer(prompt, return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "with torch.inference_mode():\n",
    "    y = model.generate(**x, max_new_tokens=120, do_sample=True, temperature=0.8, top_p=0.9)\n",
    "print(tokenizer.decode(y[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3URyTh2AGZN3",
    "outputId": "8740ae91-9935-40d0-e2f7-d694a61ec8d7"
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "ARTIFACTS = [\n",
    "    \"cpt_hi_smollm2_lora_adapters\",  # adapters folder from step 6\n",
    "    \"outputs_cpt_hi_hf\",             # trainer logs/checkpoints\n",
    "]\n",
    "\n",
    "for path in ARTIFACTS:\n",
    "    if os.path.exists(path):\n",
    "        out = path.rstrip(\"/\").replace(\"/\", \"_\") + \".zip\"\n",
    "        if os.path.isdir(path):\n",
    "            shutil.make_archive(path, 'zip', path)\n",
    "            print(\"Zipped:\", out)\n",
    "        else:\n",
    "            shutil.copy(path, out)\n",
    "            print(\"Copied:\", out)\n",
    "    else:\n",
    "        print(\"Skip (not found):\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HNepkDw_Gec4",
    "outputId": "56699894-9952-42ca-b978-171ae753ffb0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL = \"unsloth/smollm2-135m\"\n",
    "MERGED_DIR = \"cpt_hi_smollm2_merged_fp16\"\n",
    "ADAPTERS   = \"cpt_hi_smollm2_lora_adapters\"\n",
    "\n",
    "# reload base in fp16 (not 4-bit) → clean merge\n",
    "base_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "merged = PeftModel.from_pretrained(base_fp16, ADAPTERS).merge_and_unload()\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "merged.save_pretrained(MERGED_DIR)\n",
    "tok.save_pretrained(MERGED_DIR)\n",
    "print(\"✅ Merged model saved to:\", MERGED_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zxZvTc-zGf5S",
    "outputId": "54f1bca7-ab58-4315-ad9a-4502fb7b2f78"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Use adapters model (PEFT) – or switch to MERGED_DIR if you merged\n",
    "from peft import PeftModel\n",
    "BASE_MODEL = \"unsloth/smollm2-135m\"\n",
    "ADAPTERS   = \"cpt_hi_smollm2_lora_adapters\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL, device_map=\"auto\", torch_dtype=torch.float16\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, ADAPTERS)\n",
    "\n",
    "prompt = \"भारत की स्वतंत्रता के इतिहास पर तीन वाक्य लिखिए।\"\n",
    "x = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "with torch.inference_mode():\n",
    "    y = model.generate(**x, max_new_tokens=120, do_sample=True, temperature=0.8, top_p=0.9)\n",
    "print(tok.decode(y[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "utMXQv_1osOf",
    "outputId": "baee4f47-0f9c-4ee8-dfb0-c53938452684"
   },
   "outputs": [],
   "source": [
    "# Save continued-pretraining model on Hindi corpus\n",
    "SAVE_DIR = \"continued_pretraining_hindi_smollm2_final\"\n",
    "\n",
    "trainer.save_model(SAVE_DIR)           # saves model weights + config\n",
    "tokenizer.save_pretrained(SAVE_DIR)    # saves tokenizer files\n",
    "print(\"Saved to:\", SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x51bRe_fNs6c"
   },
   "source": [
    "# DPO (Direct Preference Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i8Qovi3jKrrh",
    "outputId": "bc69b38d-a4e8-4d01-a452-e30fb0d5e0e7"
   },
   "outputs": [],
   "source": [
    "!pip -q install \"transformers==4.57.1\" \"trl==0.24.0\" \\\n",
    "                \"peft==0.13.2\" \"accelerate>=0.34.2\" \\\n",
    "                \"datasets>=2.20.0\" \"bitsandbytes>=0.45.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204,
     "referenced_widgets": [
      "0c16f7bbdd644bcb9932fe7e375e6206",
      "416ea45436354f0aa10475fe59dd22a4",
      "c71482a09d3d4c76a0cfb4b4aad05e79",
      "ef59027523d740b4b07eb78f2e92e1d5",
      "48b0f097511e4e8e8e741658a09df4a8",
      "f55bfe865ff243a1b5f43adf94f01c7f",
      "7fc76d0361404fe8a1dad192292571cf",
      "277b28d8f9864592bb7dfa287c441720",
      "5c4b30c13603428aa956d64cb162417e",
      "489b90bde7f7418bade4f8027c097ae6",
      "f335713ad7c7423e9bb07b3331385e97",
      "bb46100ce3af420aa81d0f7f78d1b592",
      "e805caab29524e29950383273fd8aa66",
      "46e909a0a7754e3f8e6fdf5a33a31ae5",
      "81392eaee9bf4a6ebcaadf770ff6b04e",
      "0fc269f585a44d55850c161171e90307",
      "2a6b37d5431c4b28a5c61749e5d6acc2",
      "43242bb3f9844bdc88de4b123d3bc67b",
      "2739e4f738224155bb5500dcff61007c",
      "c97abc8165e646d689dc86cc9bd2c2b2",
      "b1d3d75128ce4e3f94ae5e50652496a5",
      "6a794b3870004b61bac4cf793dc7bd6a",
      "891a124f51c049f5a650b6e2a92cac9b",
      "afca64ad3994464d86a02817ba0d0a08",
      "bc82e94def084c2f9b08c87f0d04ef1f",
      "59a78a94456b4c8e8d98726c566b21b9",
      "30f832bb047547449f43343a087a4a1a",
      "eebd62b633ef4012a0c72dd14a51ae80",
      "2167e5b45f4c4380b16abe1f34bf5c7b",
      "df428e1f815346cf8b5328638070d39c",
      "4ab74bf2a4a24c6ba5efefe466d5a0ca",
      "b57428f13bc04a308a478187dd35a69a",
      "1b014628e9654797b9c3808c90864798"
     ]
    },
    "id": "dq7bpLJ6i-hP",
    "outputId": "7aec6bb6-4b3c-4794-d604-68b414fb7c33"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "BASE_MODEL = \"unsloth/smollm2-135m\"\n",
    "MAX_PROMPT_LEN = 256\n",
    "MAX_COMPLETION_LEN = 256\n",
    "MAX_LEN = MAX_PROMPT_LEN + MAX_COMPLETION_LEN\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.model_max_length = MAX_LEN\n",
    "\n",
    "# try 4-bit; fall back to fp16 if bitsandbytes is unavailable\n",
    "def load_model():\n",
    "    try:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        bnb_cfg = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "        )\n",
    "        print(\"✅ Loading 4-bit quantized model…\")\n",
    "        m = AutoModelForCausalLM.from_pretrained(\n",
    "            BASE_MODEL,\n",
    "            quantization_config=bnb_cfg,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ 4-bit load failed, falling back to fp16:\", type(e).__name__, \"-\", e)\n",
    "        m = AutoModelForCausalLM.from_pretrained(\n",
    "            BASE_MODEL,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "    m.config.use_cache = False\n",
    "    return m\n",
    "\n",
    "model = load_model()\n",
    "print(\"Model device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lyrNEWsolxMa",
    "outputId": "6b39bd9c-0176-46f0-d306-7b9fdea2c09f"
   },
   "outputs": [],
   "source": [
    "# --- force PEFT to not use bitsandbytes ---\n",
    "import os\n",
    "os.environ[\"BNB_TRITON_DISABLE\"] = \"1\"   # harmless even if bnb isn't present\n",
    "\n",
    "# If bnb was ever partially imported this session, nuke it:\n",
    "import sys\n",
    "for k in list(sys.modules.keys()):\n",
    "    if k.startswith(\"bitsandbytes\"):\n",
    "        sys.modules.pop(k, None)\n",
    "\n",
    "# Monkeypatch PEFT's bnb detection to always False\n",
    "import peft\n",
    "import peft.tuners.lora.model as lora_model\n",
    "lora_model.is_bnb_available = lambda: False\n",
    "\n",
    "print(\"✅ Patched PEFT: is_bnb_available() -> False\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LrGk_3x2nCi4",
    "outputId": "57a165e2-969b-4f85-eedd-afe680e91b64"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)   # <-- will not import bitsandbytes now\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510,
     "referenced_widgets": [
      "8912bad9c24546e597da0d57af0b5710",
      "d73f99bbaaf845609eacc1d103cd31fa",
      "f6e77f5d38514cd6bd278dd2258f29d0",
      "47e17f0ba0a243f8bde434e598d8019e",
      "e346d3be0a7943baaafde4296cb33fd7",
      "699a082281f14fcbba9e9b5c2dcfc25d",
      "9f6e05c3c4cb4e02bd524a4fdbbd2f4f",
      "41a63f3356384ea7bed91fcf0bfbb6cd",
      "095586737d114f6fa83059270e6c5a97",
      "4d907f0b6cc042748d14a7ec0042a42f",
      "d45e12d65dad485fb340b0b450812273",
      "4acb018b95e9430a8f136c497b52be09",
      "f405d6bb30624c5a89725c9ffe259e3b",
      "0f4f426d13644215bc0cf8e66e366123",
      "72d8982af02641bc8ce423d2b65e24e5",
      "cc221999409d446ab393fc88f90bbbc4",
      "21a86e5edeb140a1a1715954c8aa0c6f",
      "2fe0f7bae5ed48b6a2f53451fac2de72",
      "ed02449f37f4406ea6c6099a27d073c5",
      "5d95bab226f442a381e4ec7cc9b68007",
      "5e964be297d54b488a4b4ff312736385",
      "47fa4be2ff774ae58572718380f8c6fa",
      "8b0aabdbd1dd400f99e2e95e8d601451",
      "d8bcac929071434682a8468ba0165dff",
      "c6c1d7de743a46468af2b68a19632728",
      "beefa53071b04d8890fa8d8e79e44aa2",
      "733971b2100341b6b0817883360f0d2a",
      "62bdfa0abc154debbdd82a5cbbc25849",
      "8b3835ad6e8e4b8b929b02f32ebdeff9",
      "da21e68589e24f4385ba8cee315e3c08",
      "32b024da79d04ac3b84dc7dbce1e7757",
      "1e35e2e51f9f44cfa062b409ab15471f",
      "252e71216b194624b8c37b1014f8d1b2",
      "7eae50df3b4f485a970caa979d3bbba9",
      "feb8e33c4c13496cb932df9365ea0633",
      "644eeceedaff479686eed6e9b3f82ffc",
      "4824dc15560b41b28a322aeb5ab91fa6",
      "9156a69655d948eea68169307e124da7",
      "10a6906c0345451081fb9230c4a7d568",
      "f53aecae7f0c474dbc8ddb225a61440e",
      "476fc028caeb4009b0912ba564960545",
      "e013f7a2105f40dba842fbbed47156a6",
      "f5896df6021e462fac34597ec142e155",
      "bfe87d646fab41fa91eaed633d0fec1e",
      "09423e70f34d46269f674498ecbc30e9",
      "60304c4a7f51459592fbfa0e8073bd27",
      "5794e5f3e4e3459e9071072aa47c1201",
      "01b6bde9221c4f17978c59051889dfbd",
      "0b2073cdc0704dd7b192067ca58a6657",
      "8da8cd401a0d48f2a85040afe76dbd47",
      "184934e6b4e747f08b0a4d1006f653be",
      "db6a3beec655451ca16e921960297e1e",
      "aba4278e295e4c7e828123a147822f8a",
      "a885d210046f48698d65009f288aecda",
      "9c52ddc6822547f4aef8361a6ae3aeeb",
      "b9a7b3643748441ab941b93b1c80a73d",
      "2d331213ea5a4fddae16f5ccae0ae3ce",
      "442c271a438a4708b0e2e0cbbcdaa5ac",
      "fa208722942e424db6c74b35d9d207a1",
      "9a9368453f44473b922974b1d3a29d42",
      "2a8ef067c7404c20970d31ee52b43a24",
      "92f8337d2419431ab7be84da057b285a",
      "43ae484e54f046ce9eb5c3ee5597ea75",
      "fd5d508ffe38402aae865db3408daa51",
      "885f00ed4ef4475786b9345916603823",
      "87189cf91973469e8ed464d3a79f0832",
      "5d5caeeb8497410e8ec2a63879562056",
      "6748ecfbdee14273adbd8efd34c11eb9",
      "3fe3284540e74c9e8a366042d765a99f",
      "0325fabb41e645d6b669f532013e1465",
      "4d818691078e4dbd88c4b6aaaf7b3b6f",
      "bdd82e7f68ea46618e68fa96044b0477",
      "0113b860f9b7462eb5e4983896cd649d",
      "d9c0fb9137be4efcb192373d6e77b4d6",
      "80914c7ba1b746e384da8b96c68e3004",
      "8f7e26f41dfd44a5979aaf18a442ca1f",
      "b85ac5aeb1754d39ab84dd454ad1ff8b",
      "6de5f2e94dc54827a2e61e9188814ff5",
      "e7c00a0594424be5bc9c0f0f68d91627",
      "448e5b724b224a11aa8a0a783b731ae0",
      "ab92b484e79f4ac381bae3b69f4a744d",
      "28ca1ec8e89242958a77881363bca4ec",
      "0bf2ce52ec8140cf9a2eeb1b48ef3ec4",
      "c07c94aedb294f0cb7cdb52ea0a8ae75",
      "d854ea8bf51c490b9b695c225e25030a",
      "d1d9594d15e24d16a2fa8aa998bfa7fc",
      "e95f86ba68ed43878941eb4ae1a34cd4",
      "2f3cf29b6b714fb9b0eda66aa9b23751",
      "bff907e4ef0444b0a2f352b02a33bd6e",
      "f24bef80de6d431b9ced092792da1e2f",
      "ca44c8ebcf05406e935d9f494d6dae34",
      "90ce2ef6a24e4e309d83981c6653f7f7",
      "6434486c33654a2eb903812d1619aeaa",
      "41485445446d48708d80aab7cf32025b",
      "9f10310dbc6e474dac265f6c049d46d5",
      "a4f7a4ad66b24219b5c5e7cc0782679d",
      "250a77c548ce41069a9259fc1dd19e77",
      "964a109fb99f4c8e99f286c6c1ffbecc",
      "813ff335b55e4c7fafccd5910c62cc3b",
      "5169fb18ac704758a6d0c0dd8a61df43",
      "2d63f74812054531a1dad31c2f7cbf32",
      "8844b2910fb84f44abf2530a826a8f77",
      "d67a6d8549274371b989689049d84655",
      "42701f8d80d1406eab357a38fd2ec1c4",
      "05f66cf3aacd42f996f284a6304770cb",
      "50a70df8748441b6b23f62075a295489",
      "5d08837683d84f95a1c829c04597f2fa",
      "f19a518d469e407193691ec079e25ac5",
      "451b9ba992834f8a96c3f3e603ffec94",
      "8fb36d9b868c45c79f75e98d3bfc41e4",
      "7e6a263ba57045c4a84bbe979bfbf91e",
      "bed4f37280934d5e9b22812dd73c7a84",
      "e555273473614019b3e41792dd531800",
      "6ae35f7e527e46588190a24c218ceeb9",
      "b298fd41c3a7421d85cd652acdca62d4",
      "8c271224034148e09d54a8d93c6aed4a",
      "c65b0239bfdf4e32aad1fca66951ffcb",
      "1f5503b3988d4a359627ca938a9991a7",
      "1a4f3c5eaa7547dc98668abe2997f752",
      "f96458643c114b82905267d4a2f12ab7",
      "aefcd4308f24411c991991b0f2246ed1",
      "6ac59f812aab4c7988ccc5ecf7f93ee0",
      "9ea2aa344db149e98f5ca09a85e72ab5",
      "99ef0230ec4c4416a6f0dabb3de83148",
      "880a0cdd99bc4ecea9734203070d9263",
      "d10354d3c52b4f778124482ac98a0736",
      "1e066aa6c41244dca804a3c03a81b3d5",
      "2651beb654904edd86f5c1ae138a29fd",
      "fbc01e8fac69479d892a6f67c65d4426",
      "6a11f55b090b4210a343ae450ffe26bb",
      "8f73a35c91d3404c890184bf7752ffe1",
      "a5ec8ee29ae843309a10c18af335d9b6"
     ]
    },
    "id": "3jgCNapAnucL",
    "outputId": "2d7eb3f5-9778-43e9-bb47-f9727341be97"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Small, fast slice for demo; increase later (e.g., train[:8000])\n",
    "raw = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:2000]\")\n",
    "\n",
    "def add_prompt(row):\n",
    "    ch = row[\"chosen\"] or \"\"\n",
    "    first_line = ch.split(\"\\n\", 1)[0].strip()\n",
    "    row[\"prompt\"] = first_line[:200] if first_line else \"Please respond helpfully.\"\n",
    "    return row\n",
    "\n",
    "ds = raw.map(add_prompt, desc=\"Adding prompt\")\n",
    "ds = ds.select_columns([\"prompt\", \"chosen\", \"rejected\"])\n",
    "print(ds[0])\n",
    "print(\"✅ Dataset ready:\", ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EgpDKtcUoDlD",
    "outputId": "6072a10f-95c4-4974-a3e8-12417113b08a"
   },
   "outputs": [],
   "source": [
    "!pip install -q \"trl==0.8.6\" \"transformers==4.44.2\" \"accelerate\" \"datasets\" \"peft==0.12.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMqybCeQoNPP",
    "outputId": "8036a5f1-d194-4062-e736-8322ddb88de8"
   },
   "outputs": [],
   "source": [
    "import trl\n",
    "print(\"TRL version:\", trl.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eewZO3WvtxO5",
    "outputId": "b7a7c1a3-99db-4888-8b0f-4135eec10423"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "BASE_MODEL = \"unsloth/smollm2-135m\"\n",
    "\n",
    "# Force the Python (slow) tokenizer — avoids the Rust backend entirely\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    use_fast=False,          # 👈 key change\n",
    ")\n",
    "print(\"Loaded tokenizer (slow). pad_token:\", tokenizer.pad_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgX4KF99z0ao",
    "outputId": "ea8eb8b2-94f6-4738-d553-af3ff1a21b03"
   },
   "outputs": [],
   "source": [
    "# install a compatible tokenizers for transformers 4.57.1\n",
    "!pip -q install --no-deps \"tokenizers==0.20.3\"\n",
    "\n",
    "# sanity: show versions\n",
    "import importlib.metadata as im\n",
    "for p in [\"transformers\",\"tokenizers\",\"trl\",\"accelerate\",\"peft\",\"datasets\"]:\n",
    "    try: print(p, im.version(p))\n",
    "    except: print(p, \"❌ NOT INSTALLED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jyS9Se2j0H14",
    "outputId": "da662d0d-d796-41e2-f8bb-143e895ae550"
   },
   "outputs": [],
   "source": [
    "!pip -q install --no-deps --force-reinstall \"tokenizers==0.22.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yxcrTy-K0vEx",
    "outputId": "3964840f-bff8-4c7e-819d-77d6225cc203"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "BASE_MODEL = \"unsloth/smollm2-135m\"\n",
    "MAX_LEN = 512\n",
    "\n",
    "# Try fast; if it errors, switch to use_fast=False\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "except Exception:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.model_max_length = MAX_LEN\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "print(\"✅ Loaded. Device:\", next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7s8PKnM2fSc",
    "outputId": "ddce66a2-3f4b-4340-b587-0e62f4cafff6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "BASE_MODEL = \"unsloth/smollm2-135m\"\n",
    "MAX_PROMPT_LEN = 256\n",
    "MAX_COMPLETION_LEN = 256\n",
    "MAX_LEN = MAX_PROMPT_LEN + MAX_COMPLETION_LEN\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.model_max_length = MAX_LEN\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "print(\"✅ Loaded. Device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUkgyENx2j0U",
    "outputId": "13f33840-e4ad-44fd-b869-c7d33efcc5ac"
   },
   "outputs": [],
   "source": [
    "# Block any accidental bitsandbytes usage\n",
    "import os, sys\n",
    "for k in list(sys.modules.keys()):\n",
    "    if k.startswith(\"bitsandbytes\"):\n",
    "        sys.modules.pop(k, None)\n",
    "os.environ[\"BNB_TRITON_DISABLE\"] = \"1\"\n",
    "\n",
    "import peft\n",
    "import peft.import_utils as peft_import_utils\n",
    "peft_import_utils.is_bnb_available = lambda: False\n",
    "peft_import_utils.is_bnb_4bit_available = lambda: False\n",
    "try:\n",
    "    import peft.tuners.lora.model as lora_model\n",
    "    lora_model.is_bnb_available = lambda: False\n",
    "    lora_model.is_bnb_4bit_available = lambda: False\n",
    "except Exception:\n",
    "    pass\n",
    "print(\"PEFT bnb checks:\", peft_import_utils.is_bnb_available(), peft_import_utils.is_bnb_4bit_available())\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "ddbc767aa33f4d24bb54f2c74539661a",
      "437f33acf48644dc96639bc9980c74b4",
      "1566d83c14f9446594b6d08c6e42d49f",
      "18d619b14e9e4933956844f1df26e88d",
      "8f6c3828aca4409e9b5e326b777741bf",
      "d666b5e706374abbab4b6e4caa065a24",
      "5ed1be18c7e042058febff60d97a1bfc",
      "b58e341c271b4c5aa3c53097b146a289",
      "49e86f7d54584703b31194b42256efb6",
      "1fcfac1bbdf1408297ff0bcd7a32c808",
      "bc161f56f3ff446ba65d6df5c3ee758f"
     ]
    },
    "id": "ZR_61siB2n4O",
    "outputId": "5769acbc-5b32-4fb3-f63d-90fd76f2d7c0"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:2000]\")\n",
    "\n",
    "def add_prompt(row):\n",
    "    ch = row[\"chosen\"] or \"\"\n",
    "    first = ch.split(\"\\n\", 1)[0].strip()\n",
    "    row[\"prompt\"] = first[:200] if first else \"Please respond helpfully.\"\n",
    "    return row\n",
    "\n",
    "ds = raw.map(add_prompt, desc=\"Adding prompt\").select_columns([\"prompt\",\"chosen\",\"rejected\"])\n",
    "print(ds[0]); print(\"✅ Dataset size:\", len(ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VygzduJ36G8t"
   },
   "outputs": [],
   "source": [
    "!pip -q install --no-deps \"accelerate==0.34.2\"\n",
    "import os; os.kill(os.getpid(), 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "il76m3_i6RHv",
    "outputId": "1e49d2ca-f873-4861-b20b-0a7e349e6a0c"
   },
   "outputs": [],
   "source": [
    "import importlib.metadata as im\n",
    "for p in [\"transformers\",\"tokenizers\",\"trl\",\"accelerate\",\"peft\",\"datasets\"]:\n",
    "    try: print(p, im.version(p))\n",
    "    except: print(p, \"❌ NOT INSTALLED\")\n",
    "# Expect: accelerate 0.34.2 (with transformers 4.57.1, tokenizers 0.22.x, trl 0.24.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ORq6-nUx6dhT",
    "outputId": "4d16ef6e-2efc-4bfd-de89-ffb44ec371a0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "BASE_MODEL = \"unsloth/smollm2-135m\"\n",
    "MAX_PROMPT_LEN = 256\n",
    "MAX_COMPLETION_LEN = 256\n",
    "MAX_LEN = MAX_PROMPT_LEN + MAX_COMPLETION_LEN\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.model_max_length = MAX_LEN\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "print(\"✅ Loaded. Device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O94d6NTu6ou8",
    "outputId": "364bcfdc-9b9d-4127-c573-043d3dea173b"
   },
   "outputs": [],
   "source": [
    "# Block any accidental bitsandbytes usage\n",
    "import os, sys\n",
    "for k in list(sys.modules.keys()):\n",
    "    if k.startswith(\"bitsandbytes\"):\n",
    "        sys.modules.pop(k, None)\n",
    "os.environ[\"BNB_TRITON_DISABLE\"] = \"1\"\n",
    "\n",
    "import peft\n",
    "import peft.import_utils as peft_import_utils\n",
    "peft_import_utils.is_bnb_available = lambda: False\n",
    "peft_import_utils.is_bnb_4bit_available = lambda: False\n",
    "try:\n",
    "    import peft.tuners.lora.model as lora_model\n",
    "    lora_model.is_bnb_available = lambda: False\n",
    "    lora_model.is_bnb_4bit_available = lambda: False\n",
    "except Exception:\n",
    "    pass\n",
    "print(\"PEFT bnb checks:\", peft_import_utils.is_bnb_available(), peft_import_utils.is_bnb_4bit_available())\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8GdvvYq6t7f",
    "outputId": "f9253084-6232-48f2-f3b0-cf62cfd286db"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:2000]\")\n",
    "\n",
    "def add_prompt(row):\n",
    "    ch = row[\"chosen\"] or \"\"\n",
    "    first = ch.split(\"\\n\", 1)[0].strip()\n",
    "    row[\"prompt\"] = first[:200] if first else \"Please respond helpfully.\"\n",
    "    return row\n",
    "\n",
    "ds = raw.map(add_prompt, desc=\"Adding prompt\").select_columns([\"prompt\",\"chosen\",\"rejected\"])\n",
    "print(ds[0]); print(\"✅ Dataset size:\", len(ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ac136220adc24b7dbea79c8be9ac534c",
      "07898d45dac74daebb031a2b368aafc6",
      "a8dd0969e24a46658385cdbdbd54427e",
      "0fdf18472e4748caa2aff1947ddfbd13",
      "bb64c3f9bbc24443977db7c5f7f32c71",
      "caeb6f0018e04eecb599696216839939",
      "5f9e7a28d08240e98f3f63db4dd8a7d7",
      "393c639b6f5c410b9a1819fe712fbf39",
      "96859c248cd74aa6a13ca6d1c8ad0762",
      "d2c6f0eaee184499badb33e6ef537ee5",
      "9a8bbd2441a64caf89c974748ae17c50"
     ]
    },
    "id": "6yNxTrG17Vn1",
    "outputId": "0a445ab0-ffc2-4344-f89e-fb7e78e7c2e3"
   },
   "outputs": [],
   "source": [
    "# Keep the earlier ds (columns: prompt/chosen/rejected) and tokenizer\n",
    "MAX_PROMPT_LEN = 256\n",
    "MAX_COMPLETION_LEN = 256\n",
    "\n",
    "def _truncate_to(text, max_len):\n",
    "    ids = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        add_special_tokens=False,\n",
    "    )[\"input_ids\"]\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "def clip_completions(row):\n",
    "    row[\"chosen\"]   = _truncate_to(row[\"chosen\"],   MAX_COMPLETION_LEN)\n",
    "    row[\"rejected\"] = _truncate_to(row[\"rejected\"], MAX_COMPLETION_LEN)\n",
    "    return row\n",
    "\n",
    "ds = ds.map(clip_completions, desc=\"Truncating chosen/rejected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d4e4a6040bb248898d1970d096a7c087",
      "404818539eee41d39db6066428a92372",
      "22b118264c2e47f7a7032678f4db0746",
      "b16f4f7afd864e068113a30198121436",
      "efbec72268204e99b410ffcd08dceed1",
      "e535ad335fe344689e2a0c42f3595f56",
      "f8b1239ebb7449da81e8e5b81b96c231",
      "e16828315911462b8a6d6b5c0415d008",
      "99aaa545f80d4c08a9e3eb0cec2ec5de",
      "24d3fb53c5824049b755809a4e48e45f",
      "84093e0c106a430eafabce4e5e403cd9"
     ]
    },
    "id": "wWWGfObN72h2",
    "outputId": "22a8df12-a432-45ce-cdc1-dcb596c69e0f"
   },
   "outputs": [],
   "source": [
    "MAX_PROMPT_LEN = 256\n",
    "MAX_COMPLETION_LEN = 256\n",
    "\n",
    "def _truncate_to(text, max_len):\n",
    "    ids = tokenizer(text, truncation=True, max_length=max_len, add_special_tokens=False)[\"input_ids\"]\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "def clip_all(row):\n",
    "    row[\"prompt\"]   = _truncate_to(row[\"prompt\"],   MAX_PROMPT_LEN)\n",
    "    row[\"chosen\"]   = _truncate_to(row[\"chosen\"],   MAX_COMPLETION_LEN)\n",
    "    row[\"rejected\"] = _truncate_to(row[\"rejected\"], MAX_COMPLETION_LEN)\n",
    "    return row\n",
    "\n",
    "ds = ds.map(clip_all, desc=\"Clipping prompt/chosen/rejected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "1ed446d10bc244f199191d276d5384b3",
      "afc3b391d0664f9cb9cf4f1206c0bed1",
      "8afd520f4fef4020b7d303984031e4c0",
      "91516b6b00d0426e818baa8f1b29b93f",
      "0366477fd6aa45b89def67b3db8b0e61",
      "5f8aaf34e8de4718be17ff7175642475",
      "4284319a42ab422a900f9c949116e68c",
      "12c56bb25a934f6f960a6c4ddbbe832b",
      "ab3a84368a7046299dc0e71248dceb4e",
      "3ede5c3fd7dc48c3a6789266e2377db1",
      "4c7dc29d501c4020a16fe13aa63fd7d6"
     ]
    },
    "id": "VOltCz_m80N4",
    "outputId": "0db2dcb3-4375-4ba5-ee96-4dd2b2ba79c4"
   },
   "outputs": [],
   "source": [
    "def lens(row):\n",
    "    return {\n",
    "        \"prompt_len\":   len(tokenizer(row[\"prompt\"], add_special_tokens=False)[\"input_ids\"]),\n",
    "        \"chosen_len\":   len(tokenizer(row[\"chosen\"], add_special_tokens=False)[\"input_ids\"]),\n",
    "        \"rejected_len\": len(tokenizer(row[\"rejected\"], add_special_tokens=False)[\"input_ids\"]),\n",
    "    }\n",
    "\n",
    "sample = ds.select(range(32)).map(lens)\n",
    "max_prompt   = max(sample[\"prompt_len\"])\n",
    "max_chosen   = max(sample[\"chosen_len\"])\n",
    "max_rejected = max(sample[\"rejected_len\"])\n",
    "print(\"Max prompt/chosen/rejected:\", max_prompt, max_chosen, max_rejected)\n",
    "print(\"Model max:\", tokenizer.model_max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxwM8Ca--DDs"
   },
   "outputs": [],
   "source": [
    "!pip -q install --no-deps \"accelerate==0.34.2\"\n",
    "import os; os.kill(os.getpid(), 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ma5M1Aqw-JPg",
    "outputId": "eeec6a21-0c96-4a47-8395-1cbeb2f91857"
   },
   "outputs": [],
   "source": [
    "import importlib.metadata as im\n",
    "for p in [\"transformers\",\"tokenizers\",\"trl\",\"accelerate\",\"peft\",\"datasets\"]:\n",
    "    try: print(p, im.version(p))\n",
    "    except: print(p, \"❌ NOT INSTALLED\")\n",
    "# Expect: transformers 4.57.1 | tokenizers 0.22.x | trl 0.24.0 | accelerate 0.34.2 | peft 0.13.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NipFeVIi-QSn",
    "outputId": "509b3fc8-549a-4b0a-c85a-5103d58d2f53"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "BASE_MODEL = \"unsloth/smollm2-135m\"\n",
    "MAX_PROMPT_LEN, MAX_COMPLETION_LEN = 256, 256\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.model_max_length = MAX_PROMPT_LEN + MAX_COMPLETION_LEN\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "print(\"✅ Loaded. Device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VO45nyvB-czU",
    "outputId": "76e8b38f-9a9b-418e-da66-2a265d82d643"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "for k in list(sys.modules.keys()):\n",
    "    if k.startswith(\"bitsandbytes\"):\n",
    "        sys.modules.pop(k, None)\n",
    "os.environ[\"BNB_TRITON_DISABLE\"] = \"1\"\n",
    "\n",
    "import peft\n",
    "import peft.import_utils as peft_import_utils\n",
    "peft_import_utils.is_bnb_available = lambda: False\n",
    "peft_import_utils.is_bnb_4bit_available = lambda: False\n",
    "try:\n",
    "    import peft.tuners.lora.model as lora_model\n",
    "    lora_model.is_bnb_available = lambda: False\n",
    "    lora_model.is_bnb_4bit_available = lambda: False\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137,
     "referenced_widgets": [
      "320d7b12c04d4e6c9ec4b8f971999653",
      "2f0b33e704fe402bb0911d04bfb3f4f7",
      "54a8a39d680f49d889efb0111fde37f5",
      "42f9096b87c54e8082d6c833552ee799",
      "0538864c396d4a4b8f7e8727efdeea15",
      "6c781658e4164a97982028c3be824258",
      "53346142d29546f097c57306944a639a",
      "4945a5a7459b4948a8c956a7bfee6a81",
      "32dcaa5fd51d4b798ce4c1b6e8859b71",
      "1f4ca3bb3bd842cfa6bf72e3b5326144",
      "25821d5017d048cab423d1ccc654bdd2",
      "f0e9ba67846343c69d8d0b7213b30317",
      "705af8e82f244db5b13598036c1fe18a",
      "28ba9ba1d80a4021af1b117cf80c6657",
      "96da70f9065841fe82c4918c37078add",
      "cec82d7ab698478bbd888a8b7a30fcff",
      "e348d18420354821952ae516b0d9048b",
      "15258aaf9fe9437a8d2ed95271d18d0c",
      "509690ef17494b04b659b97a706aa3b3",
      "44fb32cefbd14c01ae87fd9afb4bcfdc",
      "8393d4e9e81b4bc78ab8cdafae0b4a88",
      "05c8950424c14e0082cc3cca8da91949"
     ]
    },
    "id": "gdqDgcbp-i1C",
    "outputId": "eb005407-3dee-42e9-b13b-9be6c4c5e6f7"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:2000]\")\n",
    "\n",
    "def add_prompt(row):\n",
    "    ch = row[\"chosen\"] or \"\"\n",
    "    first = ch.split(\"\\n\", 1)[0].strip()\n",
    "    row[\"prompt\"] = first[:200] if first else \"Please respond helpfully.\"\n",
    "    return row\n",
    "\n",
    "ds = raw.map(add_prompt, desc=\"Adding prompt\").select_columns([\"prompt\",\"chosen\",\"rejected\"])\n",
    "\n",
    "def _truncate_to(text, max_len):\n",
    "    ids = tokenizer(text, truncation=True, max_length=max_len, add_special_tokens=False)[\"input_ids\"]\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "def clip_all(row):\n",
    "    row[\"prompt\"]   = _truncate_to(row[\"prompt\"],   MAX_PROMPT_LEN)\n",
    "    row[\"chosen\"]   = _truncate_to(row[\"chosen\"],   MAX_COMPLETION_LEN)\n",
    "    row[\"rejected\"] = _truncate_to(row[\"rejected\"], MAX_COMPLETION_LEN)\n",
    "    return row\n",
    "\n",
    "ds = ds.map(clip_all, desc=\"Clipping prompt/chosen/rejected\")\n",
    "print(ds[0]); print(\"✅ Dataset size:\", len(ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wz0he0qOAavu"
   },
   "outputs": [],
   "source": [
    "# Make accelerate compatible with transformers 4.57.1\n",
    "!pip -q install --no-deps --force-reinstall \"accelerate==0.34.2\"\n",
    "\n",
    "# Hard-restart Python so the new wheel is actually used\n",
    "import os; os.kill(os.getpid(), 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zZuGXtqhAikO",
    "outputId": "cc6079df-db89-456d-8807-26d37bcfe06c"
   },
   "outputs": [],
   "source": [
    "import importlib.metadata as im\n",
    "for p in [\"transformers\",\"tokenizers\",\"trl\",\"accelerate\",\"peft\",\"datasets\"]:\n",
    "    try: print(p, im.version(p))\n",
    "    except: print(p, \"❌ NOT INSTALLED\")\n",
    "# Expect: transformers 4.57.1 | tokenizers 0.22.x | trl 0.24.0 | accelerate 0.34.2 | peft 0.13.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-w6BIzBB0bP"
   },
   "outputs": [],
   "source": [
    "# Define these again before your DPOConfig\n",
    "MAX_PROMPT_LEN = 256\n",
    "MAX_COMPLETION_LEN = 256\n",
    "\n",
    "from trl import DPOConfig\n",
    "\n",
    "dpo_cfg = DPOConfig(\n",
    "    output_dir=\"outputs_dpo_smollm2\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    max_steps=200,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-6,\n",
    "    beta=0.1,  # Goes inside config\n",
    "    max_prompt_length=MAX_PROMPT_LEN,\n",
    "    max_completion_length=MAX_COMPLETION_LEN,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    tf32=False,\n",
    "    report_to=[],\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOx9jrrYCNcj",
    "outputId": "67ced521-e4df-47be-810d-4d3512573d17"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "BASE_MODEL = \"unsloth/smollm2-135m\"\n",
    "MAX_PROMPT_LEN = 256\n",
    "MAX_COMPLETION_LEN = 256\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.model_max_length = MAX_PROMPT_LEN + MAX_COMPLETION_LEN\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(\"✅ Model + Tokenizer ready on\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137,
     "referenced_widgets": [
      "23ecb8e494a443c4b70a306eecdf8765",
      "7575c85236b74f0e924bb3b911470884",
      "197453713b234f949bd3ffbdfd2c5c87",
      "f58b7f5979bf4468ae0c7ad48ceb4a3d",
      "7eb278904b664fd5b4318ce39e87c1fa",
      "881beba1df3243b3a732d0f4db7e9a10",
      "6b2711b42bfa4fe1adefba3719c7aad8",
      "2eb4c9b2cfb04dd9bf49ffd39515ce56",
      "315cd45a8bd849b6a8068734a66564c1",
      "76e8a68068b8400e97e0c244209602c3",
      "5bb5522bec5e4f28b5d7b87218ee5c18",
      "62f7767dfc52416e9bbdb9dd680e7cb9",
      "f36cc81dc9ed4554941349de22a0627c",
      "028361bd3af54188a4f5199fb9e4f418",
      "049358f945154da881a4a112b0647d3c",
      "e99d13b5b8124354a2911c5b0001655a",
      "92bb42dd45de4b23ac72a9ef423a0fcc",
      "28ac4b4a56c14a31819391cf8f4eb129",
      "9750ebff9c0b4c179c62ab66265adb4e",
      "02d236b5be3f43dcada097d94962d5c4",
      "515eb5c7465145a694d7666c631ae795",
      "bb0f70df773248bbb754e8b16dac0d75"
     ]
    },
    "id": "C3mxmp8lDTLy",
    "outputId": "9a4cd5e3-19fa-49c9-b4ff-4a6c2b110df8"
   },
   "outputs": [],
   "source": [
    "# === Rebuild the tiny preference dataset for DPO ===\n",
    "from datasets import load_dataset\n",
    "\n",
    "# If tokenizer isn't in scope (after a restart), quickly reload it\n",
    "try:\n",
    "    tokenizer\n",
    "except NameError:\n",
    "    from transformers import AutoTokenizer\n",
    "    BASE_MODEL = \"unsloth/smollm2-135m\"\n",
    "    MAX_PROMPT_LEN, MAX_COMPLETION_LEN = 256, 256\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.model_max_length = MAX_PROMPT_LEN + MAX_COMPLETION_LEN\n",
    "\n",
    "raw = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:2000]\")\n",
    "\n",
    "def add_prompt(row):\n",
    "    ch = row[\"chosen\"] or \"\"\n",
    "    first = ch.split(\"\\n\", 1)[0].strip()\n",
    "    row[\"prompt\"] = first[:200] if first else \"Please respond helpfully.\"\n",
    "    return row\n",
    "\n",
    "ds = raw.map(add_prompt, desc=\"Adding prompt\").select_columns([\"prompt\",\"chosen\",\"rejected\"])\n",
    "\n",
    "def _truncate_to(text, max_len):\n",
    "    ids = tokenizer(text, truncation=True, max_length=max_len, add_special_tokens=False)[\"input_ids\"]\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "def clip_all(row):\n",
    "    row[\"prompt\"]   = _truncate_to(row[\"prompt\"],   MAX_PROMPT_LEN)\n",
    "    row[\"chosen\"]   = _truncate_to(row[\"chosen\"],   MAX_COMPLETION_LEN)\n",
    "    row[\"rejected\"] = _truncate_to(row[\"rejected\"], MAX_COMPLETION_LEN)\n",
    "    return row\n",
    "\n",
    "ds = ds.map(clip_all, desc=\"Clipping prompt/chosen/rejected\")\n",
    "print(\"✅ ds ready with columns:\", ds.column_names, \"| size:\", len(ds))\n",
    "print(ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTaU9yS4Fsa3",
    "outputId": "52cc3100-1ca5-4695-a9cd-12e89fde558c"
   },
   "outputs": [],
   "source": [
    "# === One-shot compatibility patch for accelerate < 0.34 ===\n",
    "import inspect, accelerate\n",
    "\n",
    "sig = inspect.signature(accelerate.Accelerator.unwrap_model)\n",
    "needs_patch = \"keep_torch_compile\" not in sig.parameters\n",
    "\n",
    "if needs_patch:\n",
    "    _old_unwrap = accelerate.Accelerator.unwrap_model\n",
    "    def _unwrap_compat(self, model, *args, **kwargs):\n",
    "        kwargs.pop(\"keep_torch_compile\", None)  # drop unknown kwarg\n",
    "        return _old_unwrap(self, model, *args, **kwargs)\n",
    "    accelerate.Accelerator.unwrap_model = _unwrap_compat\n",
    "    print(\"🩹 Patched accelerate.Accelerator.unwrap_model (no restart needed).\")\n",
    "else:\n",
    "    print(\"✅ accelerate already supports keep_torch_compile.\")\n",
    "\n",
    "# (Optional) show versions once so we know the runtime state\n",
    "try:\n",
    "    import importlib.metadata as im\n",
    "    print(\"versions:\",\n",
    "          \"transformers\", im.version(\"transformers\"),\n",
    "          \"| tokenizers\", im.version(\"tokenizers\"),\n",
    "          \"| trl\", im.version(\"trl\"),\n",
    "          \"| accelerate\", im.version(\"accelerate\"),\n",
    "          \"| peft\", im.version(\"peft\"))\n",
    "except Exception as _e:\n",
    "    print(\"version-check skipped:\", _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHy4nYZNKaOo",
    "outputId": "fb01152b-592b-4c39-ad1c-d9cc356842e0"
   },
   "outputs": [],
   "source": [
    "# --- Minimal collator for TRL DPOTrainer (version-proof) ---\n",
    "import torch\n",
    "\n",
    "class SimpleDPOCollator:\n",
    "    \"\"\"\n",
    "    Expects dataset items with keys: 'prompt', 'chosen', 'rejected'.\n",
    "    Returns Long tensors:\n",
    "      chosen_input_ids / chosen_attention_mask\n",
    "      rejected_input_ids / rejected_attention_mask\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, max_len: int):\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, features):\n",
    "        prompts  = [ex[\"prompt\"] for ex in features]\n",
    "        chosens  = [ex[\"chosen\"] for ex in features]\n",
    "        rejecteds= [ex[\"rejected\"] for ex in features]\n",
    "\n",
    "        # Concatenate prompt + completion (chosen/rejected)\n",
    "        chosen_texts   = [f\"{p}\\n{c}\" for p, c in zip(prompts, chosens)]\n",
    "        rejected_texts = [f\"{p}\\n{r}\" for p, r in zip(prompts, rejecteds)]\n",
    "\n",
    "        ch = self.tok(\n",
    "            chosen_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        rj = self.tok(\n",
    "            rejected_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # ensure Long dtype for embedding lookups\n",
    "        ch[\"input_ids\"] = ch[\"input_ids\"].long()\n",
    "        rj[\"input_ids\"] = rj[\"input_ids\"].long()\n",
    "        ch[\"attention_mask\"] = ch[\"attention_mask\"].long()\n",
    "        rj[\"attention_mask\"] = rj[\"attention_mask\"].long()\n",
    "\n",
    "        return {\n",
    "            \"chosen_input_ids\":   ch[\"input_ids\"],\n",
    "            \"chosen_attention_mask\": ch[\"attention_mask\"],\n",
    "            \"rejected_input_ids\": rj[\"input_ids\"],\n",
    "            \"rejected_attention_mask\": rj[\"attention_mask\"],\n",
    "        }\n",
    "\n",
    "# build the collator\n",
    "MAX_TOTAL_LEN = tokenizer.model_max_length  # e.g. 512\n",
    "collator = SimpleDPOCollator(tokenizer, MAX_TOTAL_LEN)\n",
    "print(\"✅ SimpleDPOCollator ready (max_len =\", MAX_TOTAL_LEN, \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6xZqZYEKlbK",
    "outputId": "8d54a8d3-0d39-4591-e151-e42df892012e"
   },
   "outputs": [],
   "source": [
    "# Patch unwrap_model only if your accelerate doesn't know the kwarg\n",
    "import inspect, accelerate\n",
    "if \"keep_torch_compile\" not in inspect.signature(accelerate.Accelerator.unwrap_model).parameters:\n",
    "    _old = accelerate.Accelerator.unwrap_model\n",
    "    def _unwrap_compat(self, model, *args, **kwargs):\n",
    "        kwargs.pop(\"keep_torch_compile\", None)\n",
    "        return _old(self, model, *args, **kwargs)\n",
    "    accelerate.Accelerator.unwrap_model = _unwrap_compat\n",
    "    print(\"🩹 Patched accelerate.Accelerator.unwrap_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99,
     "referenced_widgets": [
      "3fbfb433fc534ed7a57a36a4ce835776",
      "b28c9e38042942008369ee60dbc412e5",
      "48d42f177bfd4a09b1855a383d1e89ca",
      "5f7900db082a495bad76ed6cf1945145",
      "91177bece0f842e79cdbc3dd43ac363f",
      "e935fb664bca414b8b4f7436c84d024f",
      "b068074b91d74de2bbcdcb1986ffc7af",
      "8ce34bf45ad34acfb0703f09b42c7ac5",
      "478037717b5f419eaa8b214d1fb06ea5",
      "6f72751eb01c4d138317dd1afedaa282",
      "430a5f6ae9b04d179ac86b396b4784dd",
      "5c5ceb352b5f4fc286ddc9b812e491b6",
      "8d95d9ce149c4c8899d564003ab82084",
      "5dde8703548e407097941318e3ff9d60",
      "ed6595b1dfe9497cb3e2839e3f01855d",
      "b56e43ad576649eeb6a673c71b79961a",
      "33051a6e7c514c779761b6d132687c2e",
      "2d301a00b2544f378915064b055e792c",
      "2ef6310278424be282c09cb7ec55bfd9",
      "05175114e6e74522a174b2ceaa46cf7f",
      "9ac9fddcbd264abf80ffbf3f033a4ac9",
      "f08c37a30f114d5393ce6db4b64c4d92"
     ]
    },
    "id": "uRXt1F7KNRuG",
    "outputId": "b2fd51fe-c1ec-4aaf-f29b-696435eabd1c"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# If not already in memory, re-create tokenizer, model_max_length, etc.\n",
    "try:\n",
    "    tokenizer\n",
    "except NameError:\n",
    "    from transformers import AutoTokenizer\n",
    "    BASE_MODEL = \"unsloth/smollm2-135m\"\n",
    "    MAX_PROMPT_LEN, MAX_COMPLETION_LEN = 256, 256\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.model_max_length = MAX_PROMPT_LEN + MAX_COMPLETION_LEN\n",
    "\n",
    "raw = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:2000]\")\n",
    "\n",
    "def add_prompt(row):\n",
    "    ch = row[\"chosen\"] or \"\"\n",
    "    first = ch.split(\"\\n\", 1)[0].strip()\n",
    "    row[\"prompt\"] = first[:200] if first else \"Please respond helpfully.\"\n",
    "    return row\n",
    "\n",
    "ds_text = raw.map(add_prompt).select_columns([\"prompt\",\"chosen\",\"rejected\"])\n",
    "\n",
    "def _truncate_to(text, max_len):\n",
    "    ids = tokenizer(text, truncation=True, max_length=max_len, add_special_tokens=False)[\"input_ids\"]\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "def clip_all(row):\n",
    "    row[\"prompt\"]   = _truncate_to(row[\"prompt\"],   MAX_PROMPT_LEN)\n",
    "    row[\"chosen\"]   = _truncate_to(row[\"chosen\"],   MAX_COMPLETION_LEN)\n",
    "    row[\"rejected\"] = _truncate_to(row[\"rejected\"], MAX_COMPLETION_LEN)\n",
    "    return row\n",
    "\n",
    "ds_text = ds_text.map(clip_all, desc=\"Clipping prompt/chosen/rejected\")\n",
    "print(\"✅ ds_text columns:\", ds_text.column_names, \"| size:\", len(ds_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENCGAuGEUfW9"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "BASE_MODEL = \"unsloth/smollm2-135m\"\n",
    "\n",
    "# ⬇️ key change: use_fast=False\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155,
     "referenced_widgets": [
      "0aac853638e74700aa0c2f0386e67980",
      "f8b4c157562c4bde977ff4241ab5de6e",
      "90d1b22a92054cc28f27197f81c80e8c",
      "e26e6536c3f94e989256a2751ff16687",
      "05019d991a3d4537b12f99ee18873950",
      "835c648c0a004e96b6d91f6e338f835a",
      "6ebb3a1197c34bc89caad013fd5a811b",
      "36fb4cd2f4e647ca86c4d36a9de4d9a6",
      "74b7ae67f6cf433780dabfcb15301d74",
      "d01a9ec7c0314cbbb79330fdb726df80",
      "b70c7184f9e24446904cd9a24d8bc3f5",
      "877e43fb1013459394b11099069f034b",
      "4d357afda88d4248936e48227ae27af5",
      "f2dcf9831f43433a9ff4e63a011451ab",
      "1fa3161d7f644a0cb1639d9504668da2",
      "b7835e670398416f99b6cea9ce8a0a7c",
      "a8dbfe7f9f02457ab62375f14b5b9d95",
      "48f52e5689374b528c205d2c0216d28c",
      "61a03094e6fb4d44b56a0fb0120c4d5a",
      "0a436432b3214cc983f1c1a05a52f2a3",
      "4e00c333f6bc4bb2963a4bbf489118ed",
      "2c8c9c2fd6d8490f86b416516d17c347"
     ]
    },
    "id": "4LXZ5oQcU0bi",
    "outputId": "fec5c75b-37a5-4354-9b98-d78311dbda80"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Small subset of HH-RLHF dataset\n",
    "raw = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:2000]\")\n",
    "\n",
    "# Add a short \"prompt\" extracted from the chosen text\n",
    "def add_prompt(row):\n",
    "    ch = row[\"chosen\"] or \"\"\n",
    "    first = ch.split(\"\\n\", 1)[0].strip()\n",
    "    row[\"prompt\"] = first[:200] if first else \"Please respond helpfully.\"\n",
    "    return row\n",
    "\n",
    "ds = raw.map(add_prompt).select_columns([\"prompt\", \"chosen\", \"rejected\"])\n",
    "print(\"✅ Dataset columns:\", ds.column_names)\n",
    "print(\"Example:\\n\", ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HECiXYvW9pD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "MAX_PROMPT_LEN, MAX_COMPLETION_LEN = 256, 256  # keep same as earlier\n",
    "\n",
    "def dpo_collator(features):\n",
    "    # Text lists\n",
    "    prompts   = [f[\"prompt\"]   for f in features]\n",
    "    chosens   = [f[\"chosen\"]   for f in features]\n",
    "    rejecteds = [f[\"rejected\"] for f in features]\n",
    "\n",
    "    # Tokenize & pad each field separately\n",
    "    p = tokenizer(\n",
    "        prompts, padding=True, truncation=True,\n",
    "        max_length=MAX_PROMPT_LEN, return_tensors=\"pt\"\n",
    "    )\n",
    "    c = tokenizer(\n",
    "        chosens, padding=True, truncation=True,\n",
    "        max_length=MAX_COMPLETION_LEN, return_tensors=\"pt\"\n",
    "    )\n",
    "    r = tokenizer(\n",
    "        rejecteds, padding=True, truncation=True,\n",
    "        max_length=MAX_COMPLETION_LEN, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Return the exact keys DPOTrainer expects when no tokenizer/processing_class is given\n",
    "    return {\n",
    "        \"prompt_input_ids\":         p[\"input_ids\"],\n",
    "        \"prompt_attention_mask\":    p[\"attention_mask\"],\n",
    "        \"chosen_input_ids\":         c[\"input_ids\"],\n",
    "        \"chosen_attention_mask\":    c[\"attention_mask\"],\n",
    "        \"rejected_input_ids\":       r[\"input_ids\"],\n",
    "        \"rejected_attention_mask\":  r[\"attention_mask\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c3677cff00ce4d82854ae22011e790c4",
      "35f5b2f621c44ed28bf7f7963407eaf4",
      "ca45bfbafb31428a838626cb4fb3402d",
      "71c188ea573a45f6a38713f0c686521c",
      "6d6c035179ed4143800fbf8d3fe0561a",
      "44680a6e918e47049815376383f6e3c6",
      "86266d6ade454c449fcedb79ace67aab",
      "8d02c59fd6834fb997c41f1faca6b6d5",
      "7adb1d586e5244eea806cdbb0bc2c170",
      "eaaa9d40d7b94e1eab0ac764aafad9be",
      "7a1dcbbc457846b5b3a593a113edc62e"
     ]
    },
    "id": "TVmpEQGDbAYH",
    "outputId": "7cd7733b-b6f6-46b3-fbfa-0cd9619d1a0f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import DPOTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# --- if you *don't* already have dpo_cfg, uncomment this minimal one ---\n",
    "# dpo_cfg = TrainingArguments(\n",
    "#     output_dir=\"outputs_dpo_smollm2\",\n",
    "#     per_device_train_batch_size=4,   # if OOM: 2\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     num_train_epochs=1,\n",
    "#     logging_steps=10,\n",
    "#     save_steps=50,\n",
    "#     learning_rate=5e-6,\n",
    "#     fp16=False, bf16=False,          # we disable AMP\n",
    "#     report_to=[],\n",
    "#     remove_unused_columns=False,\n",
    "#     optim=\"adamw_torch\",\n",
    "# )\n",
    "\n",
    "MAX_PROMPT_LEN = 256\n",
    "MAX_COMPLETION_LEN = 256\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=dpo_cfg,              # your TrainingArguments from before (fp16=False)\n",
    "    train_dataset=ds,          # columns: prompt / chosen / rejected\n",
    "    tokenizer=tokenizer,\n",
    "    max_prompt_length=MAX_PROMPT_LEN,\n",
    "    max_target_length=MAX_COMPLETION_LEN,\n",
    "    max_length=MAX_PROMPT_LEN + MAX_COMPLETION_LEN,\n",
    "    precompute_ref_log_probs=False,\n",
    ")\n",
    "\n",
    "# ------------------ HARD DISABLE GradScaler / AMP in Accelerate ------------------\n",
    "# Some Accelerate/Trainer combos still build a GradScaler. We disable it safely.\n",
    "acc = trainer.accelerator\n",
    "\n",
    "# 1) Turn off the scaler if present\n",
    "if getattr(acc, \"scaler\", None) is not None:\n",
    "    try:\n",
    "        # Preferred: mark scaler as disabled\n",
    "        acc.scaler._enabled = False\n",
    "    except Exception:\n",
    "        # Fallback: remove reference\n",
    "        acc.scaler = None\n",
    "\n",
    "# 2) No-op unscale to avoid \"Attempting to unscale FP16 gradients.\"\n",
    "def _noop_unscale(*args, **kwargs):\n",
    "    return None\n",
    "acc.unscale_gradients = _noop_unscale\n",
    "\n",
    "# 3) Make sure no autocast sneaks in\n",
    "try:\n",
    "    torch.set_autocast_enabled(False)  # works on newer torch; harmless otherwise\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 4) Ensure model/optimizer are in fp32\n",
    "model.to(torch.float32)\n",
    "\n",
    "print(\"🚀 Starting DPO training (AMP force-disabled)…\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iqo4Ry-NghYg",
    "outputId": "6533639a-95f7-4545-e2c2-83334260c4d0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TextStreamer\n",
    "\n",
    "gen_kwargs = dict(\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.05,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "def chat(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(trainer.model.device)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    with torch.no_grad():\n",
    "        out = trainer.model.generate(**inputs, streamer=streamer, **gen_kwargs)\n",
    "    print(\"\\n\")\n",
    "\n",
    "test_prompts = [\n",
    "    \"Explain what DPO (Direct Preference Optimization) is in 2-3 sentences.\",\n",
    "    \"Write a short, friendly email to thank a mentor for their help.\",\n",
    "]\n",
    "\n",
    "for p in test_prompts:\n",
    "    print(\"### PROMPT:\", p)\n",
    "    chat(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XsqgHdJagsDT",
    "outputId": "cd1129e4-a088-45cc-c372-729cf80f2cb1"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch, copy\n",
    "\n",
    "BASE_MODEL = \"unsloth/smollm2-135m\"  # the same one you trained from\n",
    "\n",
    "base_tok = tokenizer  # reuse\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=torch.float32).to(\"cuda\")\n",
    "\n",
    "def generate(model, tok, prompt):\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(**inputs, max_new_tokens=150, do_sample=True, temperature=0.7, top_p=0.9, pad_token_id=tok.eos_token_id)\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "cmp_prompt = \"Give three tips to stay focused while studying.\"\n",
    "\n",
    "print(\"----- BASE -----\")\n",
    "print(generate(base_model, base_tok, cmp_prompt))\n",
    "\n",
    "print(\"\\n----- DPO-TUNED -----\")\n",
    "print(generate(trainer.model, tokenizer, cmp_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UGlCxD5g0lg",
    "outputId": "98d5350d-a14b-4773-95eb-e2c895c8ab2d"
   },
   "outputs": [],
   "source": [
    "SAVE_DIR = \"dpo_smollm2_final\"\n",
    "trainer.save_model(SAVE_DIR)           # saves model weights, config\n",
    "tokenizer.save_pretrained(SAVE_DIR)    # saves tokenizer\n",
    "print(\"Saved to:\", SAVE_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
